{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "import glob\n",
    "import os\n",
    "import time\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from envs import make_env "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model was made by @ikostrikov. I simply streamlined it and removed the dependency on OpenAI baselines package in an effort to:\n",
    "1) See what was going on under the hood\n",
    "2) Have an easy-to-use model for experiments in transfer learning\n",
    "3) Remove dependency on Baselines, which was causing some problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class args:\n",
    "    def __init__(self):\n",
    "        self.env_name='PongNoFrameskip-v4'\n",
    "        self.seed=1\n",
    "        self.log_dir=''\n",
    "        self.save_dir='saved_models'\n",
    "        self.cuda=True\n",
    "        self.num_stack=4\n",
    "        self.num_steps=5\n",
    "        self.num_processes=16\n",
    "        self.lr=7e-4\n",
    "        self.eps=1e-5\n",
    "        self.alpha=.99\n",
    "        self.max_grad_norm=.5\n",
    "        self.value_loss_coef=.5\n",
    "        self.entropy_coef=.1\n",
    "        self.num_frames=8e6\n",
    "        self.use_gae=False\n",
    "        self.gamma=.99\n",
    "        self.tau=.95\n",
    "        self.save_interval=1000\n",
    "        self.log_interval=100\n",
    "        self.vis_interval=100\n",
    "        self.load_model=True\n",
    "        self.save_model=False\n",
    "        \n",
    "args = args()\n",
    "\n",
    "SAVE_PATH = \"saved_models/a2c_121717.pt\"\n",
    "LOAD_PATH = \"saved_models/a2c_121717.pt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_updates = int(args.num_frames) // args.num_steps // args.num_processes\n",
    "\n",
    "torch.manual_seed(args.seed)\n",
    "if args.cuda:\n",
    "    torch.cuda.manual_seed(args.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updates 0, num timesteps 80, FPS 44, mean/median reward 0.0/0.0, min/max reward 0.0/0.0, entropy 0.22002, value loss 276.99185, policy loss -2.29684\n",
      "Updates 100, num timesteps 8080, FPS 808, mean/median reward 0.0/0.0, min/max reward 0.0/0.0, entropy 1.78344, value loss 0.12969, policy loss -0.02170\n",
      "Updates 200, num timesteps 16080, FPS 872, mean/median reward -17.9/-21.0, min/max reward -21.0/0.0, entropy 1.78924, value loss 0.01250, policy loss 0.04140\n",
      "Updates 300, num timesteps 24080, FPS 902, mean/median reward -20.4/-21.0, min/max reward -21.0/-19.0, entropy 1.78281, value loss 0.08495, policy loss -0.04526\n",
      "Updates 400, num timesteps 32080, FPS 911, mean/median reward -20.4/-21.0, min/max reward -21.0/-19.0, entropy 1.78493, value loss 0.08840, policy loss -0.09418\n",
      "Updates 500, num timesteps 40080, FPS 922, mean/median reward -20.4/-21.0, min/max reward -21.0/-19.0, entropy 1.78297, value loss 0.06992, policy loss 0.17489\n",
      "Updates 600, num timesteps 48080, FPS 926, mean/median reward -20.2/-21.0, min/max reward -21.0/-17.0, entropy 1.79065, value loss 0.01646, policy loss 0.09067\n",
      "Updates 700, num timesteps 56080, FPS 931, mean/median reward -20.1/-21.0, min/max reward -21.0/-17.0, entropy 1.79036, value loss 0.01986, policy loss -0.08138\n",
      "Updates 800, num timesteps 64080, FPS 933, mean/median reward -20.4/-21.0, min/max reward -21.0/-17.0, entropy 1.78680, value loss 0.07856, policy loss -0.06173\n",
      "Updates 900, num timesteps 72080, FPS 936, mean/median reward -20.3/-21.0, min/max reward -21.0/-17.0, entropy 1.79022, value loss 0.05955, policy loss -0.10074\n",
      "Updates 1000, num timesteps 80080, FPS 935, mean/median reward -20.1/-20.0, min/max reward -21.0/-19.0, entropy 1.78515, value loss 0.03715, policy loss -0.01218\n",
      "Updates 1100, num timesteps 88080, FPS 936, mean/median reward -20.2/-20.0, min/max reward -21.0/-19.0, entropy 1.78643, value loss 0.04363, policy loss -0.01771\n",
      "Updates 1200, num timesteps 96080, FPS 937, mean/median reward -20.4/-21.0, min/max reward -21.0/-19.0, entropy 1.78652, value loss 0.02111, policy loss 0.05510\n",
      "Updates 1300, num timesteps 104080, FPS 939, mean/median reward -20.5/-21.0, min/max reward -21.0/-19.0, entropy 1.78975, value loss 0.03552, policy loss -0.01876\n",
      "Updates 1400, num timesteps 112080, FPS 939, mean/median reward -20.7/-21.0, min/max reward -21.0/-20.0, entropy 1.79057, value loss 0.08477, policy loss -0.14308\n",
      "Updates 1500, num timesteps 120080, FPS 941, mean/median reward -20.6/-21.0, min/max reward -21.0/-19.0, entropy 1.79026, value loss 0.03507, policy loss 0.00775\n",
      "Updates 1600, num timesteps 128080, FPS 943, mean/median reward -20.2/-21.0, min/max reward -21.0/-18.0, entropy 1.79036, value loss 0.00754, policy loss -0.04541\n",
      "Updates 1700, num timesteps 136080, FPS 945, mean/median reward -20.4/-21.0, min/max reward -21.0/-18.0, entropy 1.78953, value loss 0.11389, policy loss -0.15649\n",
      "Updates 1800, num timesteps 144080, FPS 947, mean/median reward -20.1/-20.0, min/max reward -21.0/-18.0, entropy 1.78972, value loss 0.08917, policy loss 0.04899\n",
      "Updates 1900, num timesteps 152080, FPS 948, mean/median reward -20.1/-20.0, min/max reward -21.0/-18.0, entropy 1.79103, value loss 0.03800, policy loss -0.03537\n",
      "Updates 2000, num timesteps 160080, FPS 949, mean/median reward -20.1/-20.0, min/max reward -21.0/-18.0, entropy 1.79072, value loss 0.01711, policy loss -0.05732\n",
      "Updates 2100, num timesteps 168080, FPS 949, mean/median reward -20.1/-20.0, min/max reward -21.0/-19.0, entropy 1.78598, value loss 0.01379, policy loss -0.09300\n",
      "Updates 2200, num timesteps 176080, FPS 950, mean/median reward -20.4/-21.0, min/max reward -21.0/-18.0, entropy 1.78952, value loss 0.05308, policy loss -0.15682\n",
      "Updates 2300, num timesteps 184080, FPS 950, mean/median reward -20.2/-20.0, min/max reward -21.0/-18.0, entropy 1.78604, value loss 0.04123, policy loss 0.12735\n",
      "Updates 2400, num timesteps 192080, FPS 950, mean/median reward -20.3/-20.0, min/max reward -21.0/-19.0, entropy 1.78960, value loss 0.01215, policy loss -0.04756\n",
      "Updates 2500, num timesteps 200080, FPS 950, mean/median reward -20.2/-20.0, min/max reward -21.0/-19.0, entropy 1.79069, value loss 0.01320, policy loss -0.05704\n",
      "Updates 2600, num timesteps 208080, FPS 950, mean/median reward -20.4/-20.0, min/max reward -21.0/-19.0, entropy 1.78859, value loss 0.02992, policy loss 0.01581\n",
      "Updates 2700, num timesteps 216080, FPS 950, mean/median reward -20.5/-21.0, min/max reward -21.0/-19.0, entropy 1.79015, value loss 0.01598, policy loss 0.00369\n",
      "Updates 2800, num timesteps 224080, FPS 950, mean/median reward -20.3/-21.0, min/max reward -21.0/-19.0, entropy 1.78913, value loss 0.05113, policy loss 0.07671\n",
      "Updates 2900, num timesteps 232080, FPS 950, mean/median reward -20.4/-21.0, min/max reward -21.0/-19.0, entropy 1.79001, value loss 0.01609, policy loss 0.05431\n",
      "Updates 3000, num timesteps 240080, FPS 950, mean/median reward -20.2/-21.0, min/max reward -21.0/-18.0, entropy 1.78961, value loss 0.01177, policy loss -0.03041\n",
      "Updates 3100, num timesteps 248080, FPS 950, mean/median reward -20.2/-21.0, min/max reward -21.0/-18.0, entropy 1.78715, value loss 0.01144, policy loss 0.02818\n",
      "Updates 3200, num timesteps 256080, FPS 949, mean/median reward -20.1/-20.0, min/max reward -21.0/-18.0, entropy 1.79001, value loss 0.05021, policy loss -0.00573\n",
      "Updates 3300, num timesteps 264080, FPS 950, mean/median reward -20.1/-20.0, min/max reward -21.0/-18.0, entropy 1.79111, value loss 0.01052, policy loss -0.01591\n",
      "Updates 3400, num timesteps 272080, FPS 950, mean/median reward -20.1/-20.0, min/max reward -21.0/-18.0, entropy 1.79104, value loss 0.02120, policy loss -0.09000\n",
      "Updates 3500, num timesteps 280080, FPS 951, mean/median reward -20.2/-20.0, min/max reward -21.0/-19.0, entropy 1.79041, value loss 0.01077, policy loss -0.01868\n",
      "Updates 3600, num timesteps 288080, FPS 951, mean/median reward -20.4/-21.0, min/max reward -21.0/-19.0, entropy 1.79129, value loss 0.01467, policy loss 0.00601\n",
      "Updates 3700, num timesteps 296080, FPS 952, mean/median reward -20.6/-21.0, min/max reward -21.0/-19.0, entropy 1.78750, value loss 0.03147, policy loss -0.08985\n",
      "Updates 3800, num timesteps 304080, FPS 952, mean/median reward -20.4/-21.0, min/max reward -21.0/-19.0, entropy 1.79154, value loss 0.04619, policy loss 0.09910\n",
      "Updates 3900, num timesteps 312080, FPS 953, mean/median reward -20.8/-21.0, min/max reward -21.0/-20.0, entropy 1.79068, value loss 0.02710, policy loss -0.00858\n",
      "Updates 4000, num timesteps 320080, FPS 954, mean/median reward -20.6/-21.0, min/max reward -21.0/-19.0, entropy 1.78901, value loss 0.01852, policy loss 0.09116\n",
      "Updates 4100, num timesteps 328080, FPS 954, mean/median reward -20.4/-21.0, min/max reward -21.0/-19.0, entropy 1.78834, value loss 0.01952, policy loss -0.01710\n",
      "Updates 4200, num timesteps 336080, FPS 955, mean/median reward -20.1/-20.0, min/max reward -21.0/-17.0, entropy 1.78859, value loss 0.02220, policy loss 0.11766\n",
      "Updates 4300, num timesteps 344080, FPS 955, mean/median reward -20.2/-20.0, min/max reward -21.0/-17.0, entropy 1.78921, value loss 0.01454, policy loss -0.04866\n",
      "Updates 4400, num timesteps 352080, FPS 956, mean/median reward -20.1/-20.0, min/max reward -21.0/-18.0, entropy 1.79046, value loss 0.00656, policy loss 0.01290\n",
      "Updates 4500, num timesteps 360080, FPS 956, mean/median reward -20.2/-20.0, min/max reward -21.0/-19.0, entropy 1.78795, value loss 0.02044, policy loss 0.00511\n",
      "Updates 4600, num timesteps 368080, FPS 957, mean/median reward -20.2/-21.0, min/max reward -21.0/-17.0, entropy 1.79075, value loss 0.01713, policy loss 0.04122\n",
      "Updates 4700, num timesteps 376080, FPS 957, mean/median reward -20.2/-21.0, min/max reward -21.0/-17.0, entropy 1.78956, value loss 0.02193, policy loss -0.05284\n",
      "Updates 4800, num timesteps 384080, FPS 958, mean/median reward -20.3/-21.0, min/max reward -21.0/-18.0, entropy 1.78985, value loss 0.01589, policy loss 0.01376\n",
      "Updates 4900, num timesteps 392080, FPS 958, mean/median reward -20.3/-20.0, min/max reward -21.0/-18.0, entropy 1.79111, value loss 0.02184, policy loss 0.04461\n",
      "Updates 5000, num timesteps 400080, FPS 958, mean/median reward -20.2/-21.0, min/max reward -21.0/-17.0, entropy 1.79055, value loss 0.00860, policy loss 0.09424\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updates 5100, num timesteps 408080, FPS 959, mean/median reward -19.9/-20.0, min/max reward -21.0/-17.0, entropy 1.78811, value loss 0.01487, policy loss -0.02550\n",
      "Updates 5200, num timesteps 416080, FPS 959, mean/median reward -20.2/-20.0, min/max reward -21.0/-18.0, entropy 1.79041, value loss 0.01049, policy loss 0.05789\n",
      "Updates 5300, num timesteps 424080, FPS 959, mean/median reward -20.4/-21.0, min/max reward -21.0/-19.0, entropy 1.78867, value loss 0.00843, policy loss -0.00056\n",
      "Updates 5400, num timesteps 432080, FPS 960, mean/median reward -20.0/-20.0, min/max reward -21.0/-17.0, entropy 1.78900, value loss 0.02422, policy loss -0.05720\n",
      "Updates 5500, num timesteps 440080, FPS 960, mean/median reward -20.0/-20.0, min/max reward -21.0/-18.0, entropy 1.79090, value loss 0.00909, policy loss -0.00639\n",
      "Updates 5600, num timesteps 448080, FPS 960, mean/median reward -20.0/-20.0, min/max reward -21.0/-18.0, entropy 1.78876, value loss 0.01732, policy loss 0.09902\n",
      "Updates 5700, num timesteps 456080, FPS 960, mean/median reward -20.3/-21.0, min/max reward -21.0/-18.0, entropy 1.79122, value loss 0.02856, policy loss -0.09391\n",
      "Updates 5800, num timesteps 464080, FPS 960, mean/median reward -20.2/-21.0, min/max reward -21.0/-17.0, entropy 1.78959, value loss 0.01254, policy loss 0.04619\n",
      "Updates 5900, num timesteps 472080, FPS 960, mean/median reward -20.0/-21.0, min/max reward -21.0/-17.0, entropy 1.79132, value loss 0.01418, policy loss -0.02246\n",
      "Updates 6000, num timesteps 480080, FPS 960, mean/median reward -20.0/-20.0, min/max reward -21.0/-18.0, entropy 1.78905, value loss 0.01869, policy loss 0.03402\n",
      "Updates 6100, num timesteps 488080, FPS 961, mean/median reward -20.2/-21.0, min/max reward -21.0/-18.0, entropy 1.79036, value loss 0.03578, policy loss -0.04864\n",
      "Updates 6200, num timesteps 496080, FPS 961, mean/median reward -19.6/-20.0, min/max reward -21.0/-18.0, entropy 1.78991, value loss 0.05168, policy loss -0.06232\n",
      "Updates 6300, num timesteps 504080, FPS 961, mean/median reward -19.4/-20.0, min/max reward -21.0/-17.0, entropy 1.79151, value loss 0.04764, policy loss 0.20080\n",
      "Updates 6400, num timesteps 512080, FPS 961, mean/median reward -19.9/-20.0, min/max reward -21.0/-17.0, entropy 1.79042, value loss 0.01868, policy loss 0.00025\n",
      "Updates 6500, num timesteps 520080, FPS 961, mean/median reward -19.8/-20.0, min/max reward -21.0/-17.0, entropy 1.78832, value loss 0.04124, policy loss 0.14453\n",
      "Updates 6600, num timesteps 528080, FPS 961, mean/median reward -20.0/-20.0, min/max reward -21.0/-17.0, entropy 1.78572, value loss 0.01464, policy loss -0.06497\n",
      "Updates 6700, num timesteps 536080, FPS 961, mean/median reward -20.2/-20.0, min/max reward -21.0/-19.0, entropy 1.79054, value loss 0.02856, policy loss -0.08012\n",
      "Updates 6800, num timesteps 544080, FPS 962, mean/median reward -20.3/-20.0, min/max reward -21.0/-19.0, entropy 1.78950, value loss 0.02476, policy loss -0.06196\n",
      "Updates 6900, num timesteps 552080, FPS 962, mean/median reward -20.4/-21.0, min/max reward -21.0/-19.0, entropy 1.78895, value loss 0.01088, policy loss -0.05963\n",
      "Updates 7000, num timesteps 560080, FPS 962, mean/median reward -20.2/-20.0, min/max reward -21.0/-19.0, entropy 1.79056, value loss 0.02636, policy loss 0.04125\n",
      "Updates 7100, num timesteps 568080, FPS 962, mean/median reward -20.2/-20.0, min/max reward -21.0/-19.0, entropy 1.79012, value loss 0.02160, policy loss -0.01717\n",
      "Updates 7200, num timesteps 576080, FPS 962, mean/median reward -19.8/-20.0, min/max reward -21.0/-18.0, entropy 1.78871, value loss 0.03869, policy loss 0.04972\n",
      "Updates 7300, num timesteps 584080, FPS 962, mean/median reward -20.0/-20.0, min/max reward -21.0/-18.0, entropy 1.78937, value loss 0.03121, policy loss -0.02365\n",
      "Updates 7400, num timesteps 592080, FPS 962, mean/median reward -20.4/-21.0, min/max reward -21.0/-18.0, entropy 1.78839, value loss 0.03263, policy loss 0.01671\n",
      "Updates 7500, num timesteps 600080, FPS 963, mean/median reward -20.5/-21.0, min/max reward -21.0/-18.0, entropy 1.78978, value loss 0.01990, policy loss 0.01540\n",
      "Updates 7600, num timesteps 608080, FPS 963, mean/median reward -20.6/-21.0, min/max reward -21.0/-19.0, entropy 1.79000, value loss 0.02849, policy loss 0.03980\n",
      "Updates 7700, num timesteps 616080, FPS 963, mean/median reward -20.1/-20.0, min/max reward -21.0/-18.0, entropy 1.79065, value loss 0.01380, policy loss 0.04536\n",
      "Updates 7800, num timesteps 624080, FPS 963, mean/median reward -20.1/-20.0, min/max reward -21.0/-18.0, entropy 1.79106, value loss 0.01817, policy loss -0.05351\n",
      "Updates 7900, num timesteps 632080, FPS 963, mean/median reward -20.1/-20.0, min/max reward -21.0/-16.0, entropy 1.78955, value loss 0.00738, policy loss -0.01284\n",
      "Updates 8000, num timesteps 640080, FPS 963, mean/median reward -20.4/-20.0, min/max reward -21.0/-19.0, entropy 1.79029, value loss 0.05779, policy loss 0.12395\n",
      "Updates 8100, num timesteps 648080, FPS 963, mean/median reward -19.6/-20.0, min/max reward -21.0/-18.0, entropy 1.78674, value loss 0.02046, policy loss 0.04959\n",
      "Updates 8200, num timesteps 656080, FPS 963, mean/median reward -19.9/-20.0, min/max reward -21.0/-18.0, entropy 1.78883, value loss 0.01359, policy loss 0.04899\n",
      "Updates 8300, num timesteps 664080, FPS 963, mean/median reward -20.2/-20.0, min/max reward -21.0/-19.0, entropy 1.78640, value loss 0.01941, policy loss 0.05761\n",
      "Updates 8400, num timesteps 672080, FPS 963, mean/median reward -20.1/-20.0, min/max reward -21.0/-19.0, entropy 1.79005, value loss 0.02727, policy loss -0.09666\n",
      "Updates 8500, num timesteps 680080, FPS 963, mean/median reward -20.2/-20.0, min/max reward -21.0/-19.0, entropy 1.78798, value loss 0.00968, policy loss -0.03987\n",
      "Updates 8600, num timesteps 688080, FPS 963, mean/median reward -19.8/-20.0, min/max reward -21.0/-14.0, entropy 1.78886, value loss 0.02145, policy loss 0.12481\n",
      "Updates 8700, num timesteps 696080, FPS 963, mean/median reward -19.7/-20.0, min/max reward -21.0/-14.0, entropy 1.78843, value loss 0.00997, policy loss -0.03986\n",
      "Updates 8800, num timesteps 704080, FPS 963, mean/median reward -20.2/-20.0, min/max reward -21.0/-18.0, entropy 1.78581, value loss 0.01510, policy loss -0.05772\n",
      "Updates 8900, num timesteps 712080, FPS 964, mean/median reward -20.6/-21.0, min/max reward -21.0/-20.0, entropy 1.78883, value loss 0.01265, policy loss -0.03970\n",
      "Updates 9000, num timesteps 720080, FPS 964, mean/median reward -20.6/-21.0, min/max reward -21.0/-19.0, entropy 1.78520, value loss 0.02143, policy loss -0.00170\n",
      "Updates 9100, num timesteps 728080, FPS 964, mean/median reward -20.4/-21.0, min/max reward -21.0/-19.0, entropy 1.78966, value loss 0.07952, policy loss 0.11403\n",
      "Updates 9200, num timesteps 736080, FPS 964, mean/median reward -19.8/-20.0, min/max reward -21.0/-17.0, entropy 1.78770, value loss 0.02057, policy loss -0.00783\n",
      "Updates 9300, num timesteps 744080, FPS 964, mean/median reward -19.6/-20.0, min/max reward -21.0/-17.0, entropy 1.78433, value loss 0.01554, policy loss 0.00013\n",
      "Updates 9400, num timesteps 752080, FPS 964, mean/median reward -19.4/-20.0, min/max reward -21.0/-16.0, entropy 1.79058, value loss 0.01181, policy loss 0.00009\n",
      "Updates 9500, num timesteps 760080, FPS 964, mean/median reward -19.8/-20.0, min/max reward -21.0/-16.0, entropy 1.78990, value loss 0.02013, policy loss -0.05438\n",
      "Updates 9600, num timesteps 768080, FPS 964, mean/median reward -20.3/-21.0, min/max reward -21.0/-19.0, entropy 1.79031, value loss 0.01924, policy loss -0.03873\n",
      "Updates 9700, num timesteps 776080, FPS 964, mean/median reward -20.5/-21.0, min/max reward -21.0/-19.0, entropy 1.79114, value loss 0.02587, policy loss 0.03261\n",
      "Updates 9800, num timesteps 784080, FPS 965, mean/median reward -20.4/-21.0, min/max reward -21.0/-19.0, entropy 1.78533, value loss 0.01730, policy loss -0.05553\n",
      "Updates 9900, num timesteps 792080, FPS 965, mean/median reward -19.9/-20.0, min/max reward -21.0/-18.0, entropy 1.78957, value loss 0.02220, policy loss 0.03438\n",
      "Updates 10000, num timesteps 800080, FPS 965, mean/median reward -19.6/-19.0, min/max reward -21.0/-18.0, entropy 1.78908, value loss 0.01948, policy loss 0.03166\n",
      "Updates 10100, num timesteps 808080, FPS 965, mean/median reward -19.9/-20.0, min/max reward -21.0/-18.0, entropy 1.79007, value loss 0.01526, policy loss 0.03265\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updates 10200, num timesteps 816080, FPS 965, mean/median reward -19.7/-20.0, min/max reward -21.0/-18.0, entropy 1.78951, value loss 0.01650, policy loss -0.04056\n",
      "Updates 10300, num timesteps 824080, FPS 965, mean/median reward -19.9/-20.0, min/max reward -21.0/-18.0, entropy 1.78906, value loss 0.03442, policy loss -0.11058\n",
      "Updates 10400, num timesteps 832080, FPS 965, mean/median reward -20.1/-20.0, min/max reward -21.0/-16.0, entropy 1.78903, value loss 0.01202, policy loss -0.00304\n",
      "Updates 10500, num timesteps 840080, FPS 965, mean/median reward -20.3/-21.0, min/max reward -21.0/-16.0, entropy 1.78958, value loss 0.01562, policy loss 0.08803\n",
      "Updates 10600, num timesteps 848080, FPS 965, mean/median reward -20.0/-20.0, min/max reward -21.0/-19.0, entropy 1.79017, value loss 0.01205, policy loss 0.07234\n",
      "Updates 10700, num timesteps 856080, FPS 965, mean/median reward -19.7/-20.0, min/max reward -21.0/-17.0, entropy 1.78871, value loss 0.02370, policy loss -0.08772\n",
      "Updates 10800, num timesteps 864080, FPS 965, mean/median reward -19.8/-20.0, min/max reward -21.0/-17.0, entropy 1.78566, value loss 0.01966, policy loss 0.02076\n",
      "Updates 10900, num timesteps 872080, FPS 965, mean/median reward -20.2/-20.0, min/max reward -21.0/-18.0, entropy 1.78930, value loss 0.03387, policy loss 0.07189\n",
      "Updates 11000, num timesteps 880080, FPS 965, mean/median reward -20.1/-20.0, min/max reward -21.0/-18.0, entropy 1.78761, value loss 0.02551, policy loss -0.01238\n",
      "Updates 11100, num timesteps 888080, FPS 965, mean/median reward -20.3/-21.0, min/max reward -21.0/-19.0, entropy 1.78959, value loss 0.01819, policy loss 0.04644\n",
      "Updates 11200, num timesteps 896080, FPS 965, mean/median reward -20.2/-21.0, min/max reward -21.0/-18.0, entropy 1.79004, value loss 0.01218, policy loss -0.03465\n",
      "Updates 11300, num timesteps 904080, FPS 965, mean/median reward -19.9/-20.0, min/max reward -21.0/-18.0, entropy 1.78968, value loss 0.01583, policy loss -0.06254\n",
      "Updates 11400, num timesteps 912080, FPS 965, mean/median reward -19.6/-20.0, min/max reward -21.0/-18.0, entropy 1.78846, value loss 0.01879, policy loss 0.00827\n",
      "Updates 11500, num timesteps 920080, FPS 965, mean/median reward -19.4/-20.0, min/max reward -21.0/-15.0, entropy 1.78908, value loss 0.01441, policy loss -0.02483\n",
      "Updates 11600, num timesteps 928080, FPS 966, mean/median reward -19.7/-20.0, min/max reward -21.0/-15.0, entropy 1.78688, value loss 0.02217, policy loss -0.01597\n",
      "Updates 11700, num timesteps 936080, FPS 966, mean/median reward -19.9/-20.0, min/max reward -21.0/-18.0, entropy 1.78605, value loss 0.07389, policy loss 0.09690\n",
      "Updates 11800, num timesteps 944080, FPS 966, mean/median reward -20.1/-20.0, min/max reward -21.0/-18.0, entropy 1.78725, value loss 0.01891, policy loss -0.08371\n",
      "Updates 11900, num timesteps 952080, FPS 966, mean/median reward -19.6/-20.0, min/max reward -21.0/-17.0, entropy 1.78771, value loss 0.02521, policy loss -0.14370\n",
      "Updates 12000, num timesteps 960080, FPS 966, mean/median reward -19.9/-20.0, min/max reward -21.0/-18.0, entropy 1.78761, value loss 0.02692, policy loss 0.03199\n",
      "Updates 12100, num timesteps 968080, FPS 966, mean/median reward -20.1/-20.0, min/max reward -21.0/-18.0, entropy 1.78840, value loss 0.01561, policy loss -0.04709\n",
      "Updates 12200, num timesteps 976080, FPS 966, mean/median reward -20.3/-21.0, min/max reward -21.0/-18.0, entropy 1.78962, value loss 0.03783, policy loss -0.07164\n",
      "Updates 12300, num timesteps 984080, FPS 966, mean/median reward -20.2/-21.0, min/max reward -21.0/-19.0, entropy 1.78970, value loss 0.00951, policy loss 0.04673\n",
      "Updates 12400, num timesteps 992080, FPS 966, mean/median reward -20.2/-21.0, min/max reward -21.0/-19.0, entropy 1.78952, value loss 0.06037, policy loss 0.14223\n",
      "Updates 12500, num timesteps 1000080, FPS 966, mean/median reward -19.7/-20.0, min/max reward -21.0/-18.0, entropy 1.78723, value loss 0.04120, policy loss -0.01743\n",
      "Updates 12600, num timesteps 1008080, FPS 966, mean/median reward -19.8/-20.0, min/max reward -21.0/-18.0, entropy 1.78885, value loss 0.02638, policy loss -0.01769\n",
      "Updates 12700, num timesteps 1016080, FPS 966, mean/median reward -19.9/-20.0, min/max reward -21.0/-17.0, entropy 1.78632, value loss 0.03338, policy loss -0.07881\n",
      "Updates 12800, num timesteps 1024080, FPS 966, mean/median reward -19.8/-20.0, min/max reward -21.0/-17.0, entropy 1.78307, value loss 0.01847, policy loss -0.00570\n",
      "Updates 12900, num timesteps 1032080, FPS 966, mean/median reward -19.5/-20.0, min/max reward -21.0/-18.0, entropy 1.78974, value loss 0.02810, policy loss -0.00011\n",
      "Updates 13000, num timesteps 1040080, FPS 966, mean/median reward -19.5/-20.0, min/max reward -21.0/-18.0, entropy 1.78656, value loss 0.01703, policy loss -0.11372\n",
      "Updates 13100, num timesteps 1048080, FPS 966, mean/median reward -19.8/-20.0, min/max reward -21.0/-18.0, entropy 1.78086, value loss 0.02512, policy loss -0.09946\n",
      "Updates 13200, num timesteps 1056080, FPS 966, mean/median reward -19.9/-20.0, min/max reward -21.0/-18.0, entropy 1.78860, value loss 0.03410, policy loss -0.12296\n",
      "Updates 13300, num timesteps 1064080, FPS 966, mean/median reward -19.9/-20.0, min/max reward -21.0/-18.0, entropy 1.78194, value loss 0.03404, policy loss 0.05181\n",
      "Updates 13400, num timesteps 1072080, FPS 966, mean/median reward -19.9/-20.0, min/max reward -21.0/-18.0, entropy 1.78446, value loss 0.01849, policy loss -0.01927\n",
      "Updates 13500, num timesteps 1080080, FPS 966, mean/median reward -19.8/-20.0, min/max reward -21.0/-17.0, entropy 1.78653, value loss 0.02565, policy loss 0.01746\n",
      "Updates 13600, num timesteps 1088080, FPS 966, mean/median reward -19.4/-20.0, min/max reward -21.0/-17.0, entropy 1.78533, value loss 0.02892, policy loss -0.05641\n",
      "Updates 13700, num timesteps 1096080, FPS 967, mean/median reward -19.4/-20.0, min/max reward -21.0/-18.0, entropy 1.78516, value loss 0.01873, policy loss 0.02467\n",
      "Updates 13800, num timesteps 1104080, FPS 967, mean/median reward -19.5/-20.0, min/max reward -21.0/-18.0, entropy 1.78742, value loss 0.01652, policy loss -0.01794\n",
      "Updates 13900, num timesteps 1112080, FPS 967, mean/median reward -19.1/-19.0, min/max reward -21.0/-17.0, entropy 1.78584, value loss 0.01297, policy loss -0.03305\n",
      "Updates 14000, num timesteps 1120080, FPS 967, mean/median reward -19.2/-19.0, min/max reward -21.0/-17.0, entropy 1.78290, value loss 0.02572, policy loss -0.07718\n",
      "Updates 14100, num timesteps 1128080, FPS 967, mean/median reward -19.5/-20.0, min/max reward -21.0/-17.0, entropy 1.78503, value loss 0.01438, policy loss -0.00158\n",
      "Updates 14200, num timesteps 1136080, FPS 967, mean/median reward -19.9/-20.0, min/max reward -21.0/-18.0, entropy 1.78023, value loss 0.01903, policy loss 0.01212\n",
      "Updates 14300, num timesteps 1144080, FPS 967, mean/median reward -19.2/-20.0, min/max reward -21.0/-17.0, entropy 1.78190, value loss 0.04916, policy loss -0.05885\n",
      "Updates 14400, num timesteps 1152080, FPS 967, mean/median reward -19.0/-19.0, min/max reward -21.0/-17.0, entropy 1.78323, value loss 0.01523, policy loss 0.03765\n",
      "Updates 14500, num timesteps 1160080, FPS 967, mean/median reward -19.7/-20.0, min/max reward -21.0/-17.0, entropy 1.78369, value loss 0.05241, policy loss -0.14159\n",
      "Updates 14600, num timesteps 1168080, FPS 967, mean/median reward -19.5/-20.0, min/max reward -21.0/-18.0, entropy 1.78993, value loss 0.03761, policy loss 0.05901\n",
      "Updates 14700, num timesteps 1176080, FPS 967, mean/median reward -19.2/-20.0, min/max reward -20.0/-18.0, entropy 1.78212, value loss 0.01480, policy loss 0.01547\n",
      "Updates 14800, num timesteps 1184080, FPS 967, mean/median reward -18.9/-19.0, min/max reward -20.0/-17.0, entropy 1.78279, value loss 0.02053, policy loss 0.00842\n",
      "Updates 14900, num timesteps 1192080, FPS 967, mean/median reward -18.9/-19.0, min/max reward -21.0/-17.0, entropy 1.78005, value loss 0.02605, policy loss 0.04194\n",
      "Updates 15000, num timesteps 1200080, FPS 967, mean/median reward -19.1/-20.0, min/max reward -21.0/-17.0, entropy 1.78435, value loss 0.03813, policy loss 0.02206\n",
      "Updates 15100, num timesteps 1208080, FPS 967, mean/median reward -19.6/-20.0, min/max reward -21.0/-18.0, entropy 1.78230, value loss 0.01242, policy loss 0.03081\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updates 15200, num timesteps 1216080, FPS 967, mean/median reward -19.4/-20.0, min/max reward -21.0/-17.0, entropy 1.78087, value loss 0.02307, policy loss -0.03950\n",
      "Updates 15300, num timesteps 1224080, FPS 967, mean/median reward -19.2/-19.0, min/max reward -21.0/-17.0, entropy 1.78090, value loss 0.05348, policy loss 0.07946\n",
      "Updates 15400, num timesteps 1232080, FPS 967, mean/median reward -19.5/-20.0, min/max reward -21.0/-18.0, entropy 1.77885, value loss 0.02499, policy loss 0.02787\n",
      "Updates 15500, num timesteps 1240080, FPS 968, mean/median reward -19.6/-19.0, min/max reward -21.0/-19.0, entropy 1.78404, value loss 0.00853, policy loss 0.03214\n",
      "Updates 15600, num timesteps 1248080, FPS 967, mean/median reward -19.1/-19.0, min/max reward -21.0/-17.0, entropy 1.78098, value loss 0.04706, policy loss -0.00625\n",
      "Updates 15700, num timesteps 1256080, FPS 968, mean/median reward -18.7/-19.0, min/max reward -20.0/-16.0, entropy 1.78341, value loss 0.06280, policy loss 0.03279\n",
      "Updates 15800, num timesteps 1264080, FPS 968, mean/median reward -19.1/-19.0, min/max reward -21.0/-16.0, entropy 1.77895, value loss 0.02870, policy loss 0.03021\n",
      "Updates 15900, num timesteps 1272080, FPS 968, mean/median reward -19.7/-20.0, min/max reward -21.0/-18.0, entropy 1.78628, value loss 0.01569, policy loss -0.02189\n",
      "Updates 16000, num timesteps 1280080, FPS 968, mean/median reward -19.4/-20.0, min/max reward -21.0/-16.0, entropy 1.77428, value loss 0.03434, policy loss 0.02789\n",
      "Updates 16100, num timesteps 1288080, FPS 968, mean/median reward -19.1/-20.0, min/max reward -21.0/-16.0, entropy 1.76565, value loss 0.03791, policy loss -0.04982\n",
      "Updates 16200, num timesteps 1296080, FPS 968, mean/median reward -18.1/-19.0, min/max reward -20.0/-16.0, entropy 1.77510, value loss 0.02645, policy loss -0.04260\n",
      "Updates 16300, num timesteps 1304080, FPS 968, mean/median reward -18.4/-19.0, min/max reward -21.0/-14.0, entropy 1.77583, value loss 0.03883, policy loss -0.06321\n",
      "Updates 16400, num timesteps 1312080, FPS 968, mean/median reward -18.4/-19.0, min/max reward -21.0/-14.0, entropy 1.77394, value loss 0.03106, policy loss -0.00261\n",
      "Updates 16500, num timesteps 1320080, FPS 968, mean/median reward -18.4/-19.0, min/max reward -21.0/-14.0, entropy 1.77796, value loss 0.04828, policy loss -0.04001\n",
      "Updates 16600, num timesteps 1328080, FPS 968, mean/median reward -18.3/-18.0, min/max reward -21.0/-15.0, entropy 1.75871, value loss 0.03189, policy loss 0.01513\n",
      "Updates 16700, num timesteps 1336080, FPS 968, mean/median reward -18.8/-20.0, min/max reward -21.0/-15.0, entropy 1.77538, value loss 0.02021, policy loss -0.01876\n",
      "Updates 16800, num timesteps 1344080, FPS 968, mean/median reward -18.9/-20.0, min/max reward -21.0/-16.0, entropy 1.75798, value loss 0.02879, policy loss -0.00688\n",
      "Updates 16900, num timesteps 1352080, FPS 968, mean/median reward -18.7/-19.0, min/max reward -21.0/-16.0, entropy 1.78146, value loss 0.02619, policy loss -0.03099\n",
      "Updates 17000, num timesteps 1360080, FPS 968, mean/median reward -18.9/-19.0, min/max reward -21.0/-14.0, entropy 1.75287, value loss 0.02094, policy loss -0.09272\n",
      "Updates 17100, num timesteps 1368080, FPS 968, mean/median reward -18.9/-19.0, min/max reward -21.0/-14.0, entropy 1.78349, value loss 0.02927, policy loss -0.00848\n",
      "Updates 17200, num timesteps 1376080, FPS 968, mean/median reward -18.6/-19.0, min/max reward -21.0/-14.0, entropy 1.78465, value loss 0.01870, policy loss 0.01758\n",
      "Updates 17300, num timesteps 1384080, FPS 968, mean/median reward -18.4/-18.0, min/max reward -21.0/-16.0, entropy 1.77927, value loss 0.02275, policy loss -0.01565\n",
      "Updates 17400, num timesteps 1392080, FPS 969, mean/median reward -18.6/-19.0, min/max reward -20.0/-16.0, entropy 1.77560, value loss 0.02979, policy loss -0.07716\n",
      "Updates 17500, num timesteps 1400080, FPS 969, mean/median reward -18.7/-19.0, min/max reward -20.0/-16.0, entropy 1.76937, value loss 0.04295, policy loss -0.14470\n",
      "Updates 17600, num timesteps 1408080, FPS 969, mean/median reward -18.1/-19.0, min/max reward -21.0/-13.0, entropy 1.76335, value loss 0.04434, policy loss 0.06199\n",
      "Updates 17700, num timesteps 1416080, FPS 969, mean/median reward -17.7/-18.0, min/max reward -21.0/-13.0, entropy 1.76772, value loss 0.05799, policy loss 0.03997\n",
      "Updates 17800, num timesteps 1424080, FPS 969, mean/median reward -18.3/-19.0, min/max reward -21.0/-15.0, entropy 1.76810, value loss 0.01876, policy loss 0.07578\n",
      "Updates 17900, num timesteps 1432080, FPS 969, mean/median reward -18.8/-19.0, min/max reward -21.0/-16.0, entropy 1.76895, value loss 0.01403, policy loss -0.03774\n",
      "Updates 18000, num timesteps 1440080, FPS 969, mean/median reward -18.8/-19.0, min/max reward -21.0/-16.0, entropy 1.75684, value loss 0.02405, policy loss -0.12432\n",
      "Updates 18100, num timesteps 1448080, FPS 969, mean/median reward -18.3/-19.0, min/max reward -20.0/-15.0, entropy 1.77961, value loss 0.01101, policy loss 0.01041\n",
      "Updates 18200, num timesteps 1456080, FPS 969, mean/median reward -18.1/-19.0, min/max reward -20.0/-15.0, entropy 1.77352, value loss 0.03088, policy loss -0.05860\n",
      "Updates 18300, num timesteps 1464080, FPS 969, mean/median reward -18.0/-19.0, min/max reward -20.0/-15.0, entropy 1.78094, value loss 0.02509, policy loss -0.05770\n",
      "Updates 18400, num timesteps 1472080, FPS 969, mean/median reward -18.2/-19.0, min/max reward -20.0/-16.0, entropy 1.76775, value loss 0.03653, policy loss -0.07838\n",
      "Updates 18500, num timesteps 1480080, FPS 969, mean/median reward -18.3/-19.0, min/max reward -20.0/-16.0, entropy 1.75941, value loss 0.04051, policy loss 0.11747\n",
      "Updates 18600, num timesteps 1488080, FPS 969, mean/median reward -18.2/-18.0, min/max reward -20.0/-17.0, entropy 1.75815, value loss 0.00942, policy loss 0.01433\n",
      "Updates 18700, num timesteps 1496080, FPS 969, mean/median reward -18.0/-18.0, min/max reward -21.0/-16.0, entropy 1.75994, value loss 0.04006, policy loss -0.03408\n",
      "Updates 18800, num timesteps 1504080, FPS 969, mean/median reward -17.8/-18.0, min/max reward -21.0/-15.0, entropy 1.76607, value loss 0.03714, policy loss -0.03978\n",
      "Updates 18900, num timesteps 1512080, FPS 969, mean/median reward -17.9/-18.0, min/max reward -21.0/-15.0, entropy 1.77046, value loss 0.01544, policy loss 0.03035\n",
      "Updates 19000, num timesteps 1520080, FPS 969, mean/median reward -18.4/-19.0, min/max reward -21.0/-15.0, entropy 1.75585, value loss 0.02180, policy loss -0.05918\n",
      "Updates 19100, num timesteps 1528080, FPS 969, mean/median reward -18.7/-19.0, min/max reward -21.0/-16.0, entropy 1.76696, value loss 0.01452, policy loss -0.07749\n",
      "Updates 19200, num timesteps 1536080, FPS 969, mean/median reward -18.2/-19.0, min/max reward -20.0/-15.0, entropy 1.78262, value loss 0.03669, policy loss 0.02682\n",
      "Updates 19300, num timesteps 1544080, FPS 969, mean/median reward -18.3/-19.0, min/max reward -20.0/-15.0, entropy 1.75145, value loss 0.01669, policy loss 0.11683\n",
      "Updates 19400, num timesteps 1552080, FPS 969, mean/median reward -18.4/-19.0, min/max reward -20.0/-15.0, entropy 1.72386, value loss 0.03666, policy loss 0.02567\n",
      "Updates 19500, num timesteps 1560080, FPS 969, mean/median reward -18.4/-19.0, min/max reward -20.0/-14.0, entropy 1.75300, value loss 0.01684, policy loss 0.00974\n",
      "Updates 19600, num timesteps 1568080, FPS 969, mean/median reward -18.6/-19.0, min/max reward -20.0/-14.0, entropy 1.77308, value loss 0.02097, policy loss 0.00804\n",
      "Updates 19700, num timesteps 1576080, FPS 969, mean/median reward -18.4/-19.0, min/max reward -20.0/-16.0, entropy 1.77619, value loss 0.02677, policy loss -0.00073\n",
      "Updates 19800, num timesteps 1584080, FPS 969, mean/median reward -17.9/-19.0, min/max reward -20.0/-15.0, entropy 1.76124, value loss 0.02366, policy loss 0.05604\n",
      "Updates 19900, num timesteps 1592080, FPS 969, mean/median reward -17.9/-18.0, min/max reward -20.0/-15.0, entropy 1.75886, value loss 0.01180, policy loss -0.01994\n",
      "Updates 20000, num timesteps 1600080, FPS 969, mean/median reward -18.6/-19.0, min/max reward -21.0/-16.0, entropy 1.77843, value loss 0.01344, policy loss 0.01179\n",
      "Updates 20100, num timesteps 1608080, FPS 969, mean/median reward -18.7/-19.0, min/max reward -21.0/-16.0, entropy 1.77676, value loss 0.01958, policy loss 0.02324\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updates 20200, num timesteps 1616080, FPS 969, mean/median reward -18.5/-19.0, min/max reward -21.0/-16.0, entropy 1.77127, value loss 0.01239, policy loss 0.03854\n",
      "Updates 20300, num timesteps 1624080, FPS 969, mean/median reward -18.7/-19.0, min/max reward -21.0/-14.0, entropy 1.74376, value loss 0.01250, policy loss -0.04821\n",
      "Updates 20400, num timesteps 1632080, FPS 969, mean/median reward -18.2/-19.0, min/max reward -21.0/-14.0, entropy 1.78521, value loss 0.05471, policy loss 0.10010\n",
      "Updates 20500, num timesteps 1640080, FPS 969, mean/median reward -18.1/-19.0, min/max reward -21.0/-14.0, entropy 1.76032, value loss 0.02497, policy loss -0.08266\n",
      "Updates 20600, num timesteps 1648080, FPS 969, mean/median reward -17.7/-18.0, min/max reward -20.0/-14.0, entropy 1.76023, value loss 0.03277, policy loss -0.02534\n",
      "Updates 20700, num timesteps 1656080, FPS 969, mean/median reward -17.8/-18.0, min/max reward -20.0/-15.0, entropy 1.78542, value loss 0.01700, policy loss -0.02744\n",
      "Updates 20800, num timesteps 1664080, FPS 969, mean/median reward -17.8/-18.0, min/max reward -20.0/-15.0, entropy 1.76148, value loss 0.01596, policy loss -0.06630\n",
      "Updates 20900, num timesteps 1672080, FPS 969, mean/median reward -17.8/-18.0, min/max reward -20.0/-15.0, entropy 1.74686, value loss 0.01084, policy loss -0.00151\n",
      "Updates 21000, num timesteps 1680080, FPS 969, mean/median reward -17.9/-18.0, min/max reward -20.0/-13.0, entropy 1.77490, value loss 0.01039, policy loss -0.03477\n",
      "Updates 21100, num timesteps 1688080, FPS 969, mean/median reward -17.9/-18.0, min/max reward -20.0/-13.0, entropy 1.74855, value loss 0.01174, policy loss -0.01597\n",
      "Updates 21200, num timesteps 1696080, FPS 969, mean/median reward -18.0/-18.0, min/max reward -20.0/-13.0, entropy 1.72475, value loss 0.04117, policy loss -0.01497\n",
      "Updates 21300, num timesteps 1704080, FPS 969, mean/median reward -17.5/-18.0, min/max reward -20.0/-14.0, entropy 1.78165, value loss 0.01655, policy loss -0.03868\n",
      "Updates 21400, num timesteps 1712080, FPS 969, mean/median reward -17.4/-18.0, min/max reward -20.0/-14.0, entropy 1.77394, value loss 0.01926, policy loss -0.00322\n",
      "Updates 21500, num timesteps 1720080, FPS 969, mean/median reward -17.2/-17.0, min/max reward -20.0/-14.0, entropy 1.75841, value loss 0.02246, policy loss -0.00926\n",
      "Updates 21600, num timesteps 1728080, FPS 969, mean/median reward -17.2/-18.0, min/max reward -20.0/-12.0, entropy 1.73159, value loss 0.02377, policy loss -0.02012\n",
      "Updates 21700, num timesteps 1736080, FPS 969, mean/median reward -16.8/-17.0, min/max reward -20.0/-12.0, entropy 1.76610, value loss 0.03474, policy loss -0.07490\n",
      "Updates 21800, num timesteps 1744080, FPS 969, mean/median reward -16.6/-17.0, min/max reward -19.0/-12.0, entropy 1.77607, value loss 0.00667, policy loss 0.01572\n",
      "Updates 21900, num timesteps 1752080, FPS 969, mean/median reward -16.8/-17.0, min/max reward -19.0/-14.0, entropy 1.77709, value loss 0.02340, policy loss -0.03161\n",
      "Updates 22000, num timesteps 1760080, FPS 969, mean/median reward -16.8/-17.0, min/max reward -19.0/-13.0, entropy 1.75928, value loss 0.01072, policy loss -0.03523\n",
      "Updates 22100, num timesteps 1768080, FPS 969, mean/median reward -16.5/-17.0, min/max reward -19.0/-13.0, entropy 1.76088, value loss 0.01744, policy loss 0.01458\n",
      "Updates 22200, num timesteps 1776080, FPS 969, mean/median reward -16.1/-16.0, min/max reward -19.0/-13.0, entropy 1.76683, value loss 0.01969, policy loss -0.01510\n",
      "Updates 22300, num timesteps 1784080, FPS 969, mean/median reward -16.5/-17.0, min/max reward -19.0/-12.0, entropy 1.75432, value loss 0.02292, policy loss 0.09656\n",
      "Updates 22400, num timesteps 1792080, FPS 969, mean/median reward -16.1/-17.0, min/max reward -19.0/-12.0, entropy 1.77864, value loss 0.00941, policy loss 0.00283\n",
      "Updates 22500, num timesteps 1800080, FPS 969, mean/median reward -17.0/-18.0, min/max reward -21.0/-12.0, entropy 1.75741, value loss 0.03027, policy loss 0.03498\n",
      "Updates 22600, num timesteps 1808080, FPS 969, mean/median reward -16.4/-17.0, min/max reward -21.0/-12.0, entropy 1.75142, value loss 0.02191, policy loss -0.08305\n",
      "Updates 22700, num timesteps 1816080, FPS 969, mean/median reward -16.8/-17.0, min/max reward -21.0/-13.0, entropy 1.75381, value loss 0.00927, policy loss -0.00557\n",
      "Updates 22800, num timesteps 1824080, FPS 969, mean/median reward -16.8/-17.0, min/max reward -21.0/-13.0, entropy 1.75743, value loss 0.01102, policy loss 0.03118\n",
      "Updates 22900, num timesteps 1832080, FPS 969, mean/median reward -17.2/-17.0, min/max reward -20.0/-15.0, entropy 1.70811, value loss 0.02347, policy loss 0.07281\n",
      "Updates 23000, num timesteps 1840080, FPS 969, mean/median reward -17.4/-17.0, min/max reward -20.0/-15.0, entropy 1.74575, value loss 0.04764, policy loss -0.07579\n",
      "Updates 23100, num timesteps 1848080, FPS 969, mean/median reward -17.3/-17.0, min/max reward -20.0/-16.0, entropy 1.78025, value loss 0.04434, policy loss -0.02413\n",
      "Updates 23200, num timesteps 1856080, FPS 969, mean/median reward -17.0/-17.0, min/max reward -20.0/-13.0, entropy 1.73313, value loss 0.03423, policy loss -0.10026\n",
      "Updates 23300, num timesteps 1864080, FPS 969, mean/median reward -16.3/-16.0, min/max reward -19.0/-13.0, entropy 1.76012, value loss 0.02778, policy loss 0.10843\n",
      "Updates 23400, num timesteps 1872080, FPS 969, mean/median reward -16.3/-16.0, min/max reward -19.0/-13.0, entropy 1.76375, value loss 0.03044, policy loss -0.02916\n",
      "Updates 23500, num timesteps 1880080, FPS 969, mean/median reward -16.1/-16.0, min/max reward -19.0/-13.0, entropy 1.76566, value loss 0.01428, policy loss 0.01681\n",
      "Updates 23600, num timesteps 1888080, FPS 969, mean/median reward -16.9/-17.0, min/max reward -19.0/-14.0, entropy 1.75151, value loss 0.02420, policy loss -0.06990\n",
      "Updates 23700, num timesteps 1896080, FPS 969, mean/median reward -16.8/-17.0, min/max reward -19.0/-14.0, entropy 1.76451, value loss 0.01582, policy loss -0.09561\n",
      "Updates 23800, num timesteps 1904080, FPS 969, mean/median reward -16.8/-17.0, min/max reward -19.0/-14.0, entropy 1.74874, value loss 0.03403, policy loss 0.03543\n",
      "Updates 23900, num timesteps 1912080, FPS 969, mean/median reward -16.7/-17.0, min/max reward -21.0/-12.0, entropy 1.74731, value loss 0.02540, policy loss 0.06338\n",
      "Updates 24000, num timesteps 1920080, FPS 969, mean/median reward -16.6/-17.0, min/max reward -21.0/-12.0, entropy 1.78457, value loss 0.02405, policy loss 0.05995\n",
      "Updates 24100, num timesteps 1928080, FPS 969, mean/median reward -16.4/-16.0, min/max reward -21.0/-12.0, entropy 1.73343, value loss 0.03045, policy loss 0.06290\n",
      "Updates 24200, num timesteps 1936080, FPS 969, mean/median reward -16.9/-17.0, min/max reward -21.0/-13.0, entropy 1.71511, value loss 0.06466, policy loss -0.04599\n",
      "Updates 24300, num timesteps 1944080, FPS 969, mean/median reward -16.4/-16.0, min/max reward -20.0/-13.0, entropy 1.76592, value loss 0.01811, policy loss -0.04908\n",
      "Updates 24400, num timesteps 1952080, FPS 969, mean/median reward -16.2/-16.0, min/max reward -19.0/-13.0, entropy 1.74438, value loss 0.02578, policy loss 0.09144\n",
      "Updates 24500, num timesteps 1960080, FPS 969, mean/median reward -16.2/-16.0, min/max reward -19.0/-13.0, entropy 1.72149, value loss 0.02249, policy loss -0.03088\n",
      "Updates 24600, num timesteps 1968080, FPS 969, mean/median reward -16.4/-17.0, min/max reward -19.0/-13.0, entropy 1.76183, value loss 0.02402, policy loss 0.04062\n",
      "Updates 24700, num timesteps 1976080, FPS 969, mean/median reward -16.1/-16.0, min/max reward -19.0/-10.0, entropy 1.74006, value loss 0.03071, policy loss 0.09184\n",
      "Updates 24800, num timesteps 1984080, FPS 970, mean/median reward -16.0/-16.0, min/max reward -19.0/-10.0, entropy 1.73843, value loss 0.02822, policy loss 0.01713\n",
      "Updates 24900, num timesteps 1992080, FPS 970, mean/median reward -15.9/-16.0, min/max reward -20.0/-10.0, entropy 1.77481, value loss 0.01565, policy loss -0.05908\n",
      "Updates 25000, num timesteps 2000080, FPS 970, mean/median reward -15.6/-16.0, min/max reward -20.0/-11.0, entropy 1.77398, value loss 0.00663, policy loss 0.03218\n",
      "Updates 25100, num timesteps 2008080, FPS 970, mean/median reward -15.2/-15.0, min/max reward -20.0/-11.0, entropy 1.75693, value loss 0.02363, policy loss -0.00059\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updates 25200, num timesteps 2016080, FPS 970, mean/median reward -15.1/-15.0, min/max reward -20.0/-11.0, entropy 1.75234, value loss 0.05111, policy loss -0.04186\n",
      "Updates 25300, num timesteps 2024080, FPS 970, mean/median reward -15.4/-16.0, min/max reward -18.0/-12.0, entropy 1.75428, value loss 0.00884, policy loss 0.00004\n",
      "Updates 25400, num timesteps 2032080, FPS 970, mean/median reward -15.9/-16.0, min/max reward -18.0/-12.0, entropy 1.76341, value loss 0.02214, policy loss -0.06472\n",
      "Updates 25500, num timesteps 2040080, FPS 970, mean/median reward -16.4/-17.0, min/max reward -20.0/-10.0, entropy 1.75968, value loss 0.10450, policy loss -0.03081\n",
      "Updates 25600, num timesteps 2048080, FPS 970, mean/median reward -16.7/-17.0, min/max reward -20.0/-10.0, entropy 1.75934, value loss 0.02588, policy loss -0.09025\n",
      "Updates 25700, num timesteps 2056080, FPS 970, mean/median reward -16.4/-17.0, min/max reward -20.0/-10.0, entropy 1.78018, value loss 0.00769, policy loss 0.02153\n",
      "Updates 25800, num timesteps 2064080, FPS 970, mean/median reward -16.6/-17.0, min/max reward -19.0/-13.0, entropy 1.77210, value loss 0.01672, policy loss -0.12276\n",
      "Updates 25900, num timesteps 2072080, FPS 970, mean/median reward -16.2/-16.0, min/max reward -19.0/-13.0, entropy 1.76103, value loss 0.01417, policy loss -0.03584\n",
      "Updates 26000, num timesteps 2080080, FPS 970, mean/median reward -16.5/-16.0, min/max reward -19.0/-13.0, entropy 1.76293, value loss 0.01212, policy loss -0.06884\n",
      "Updates 26100, num timesteps 2088080, FPS 970, mean/median reward -16.6/-17.0, min/max reward -19.0/-13.0, entropy 1.75194, value loss 0.01735, policy loss 0.02670\n",
      "Updates 26200, num timesteps 2096080, FPS 970, mean/median reward -16.2/-17.0, min/max reward -19.0/-10.0, entropy 1.77810, value loss 0.02722, policy loss -0.03876\n",
      "Updates 26300, num timesteps 2104080, FPS 970, mean/median reward -16.4/-17.0, min/max reward -19.0/-10.0, entropy 1.72598, value loss 0.03137, policy loss 0.09467\n",
      "Updates 26400, num timesteps 2112080, FPS 970, mean/median reward -15.9/-16.0, min/max reward -19.0/-10.0, entropy 1.77493, value loss 0.02326, policy loss 0.07092\n",
      "Updates 26500, num timesteps 2120080, FPS 970, mean/median reward -15.4/-16.0, min/max reward -19.0/-10.0, entropy 1.74942, value loss 0.03049, policy loss 0.02875\n",
      "Updates 26600, num timesteps 2128080, FPS 970, mean/median reward -15.5/-16.0, min/max reward -18.0/-11.0, entropy 1.78057, value loss 0.00806, policy loss -0.02884\n",
      "Updates 26700, num timesteps 2136080, FPS 970, mean/median reward -15.9/-17.0, min/max reward -20.0/-11.0, entropy 1.75209, value loss 0.02510, policy loss -0.03460\n",
      "Updates 26800, num timesteps 2144080, FPS 970, mean/median reward -15.5/-15.0, min/max reward -20.0/-11.0, entropy 1.73822, value loss 0.01311, policy loss 0.01661\n",
      "Updates 26900, num timesteps 2152080, FPS 970, mean/median reward -16.4/-17.0, min/max reward -20.0/-13.0, entropy 1.74116, value loss 0.01874, policy loss -0.00918\n",
      "Updates 27000, num timesteps 2160080, FPS 970, mean/median reward -16.8/-18.0, min/max reward -20.0/-13.0, entropy 1.77797, value loss 0.01607, policy loss 0.04026\n",
      "Updates 27100, num timesteps 2168080, FPS 970, mean/median reward -16.7/-17.0, min/max reward -20.0/-13.0, entropy 1.78616, value loss 0.02551, policy loss 0.01572\n",
      "Updates 27200, num timesteps 2176080, FPS 970, mean/median reward -16.9/-17.0, min/max reward -20.0/-14.0, entropy 1.78530, value loss 0.00976, policy loss -0.01990\n",
      "Updates 27300, num timesteps 2184080, FPS 970, mean/median reward -16.8/-17.0, min/max reward -20.0/-14.0, entropy 1.76808, value loss 0.01590, policy loss -0.06859\n",
      "Updates 27400, num timesteps 2192080, FPS 970, mean/median reward -16.3/-17.0, min/max reward -20.0/-11.0, entropy 1.73366, value loss 0.02044, policy loss 0.03405\n",
      "Updates 27500, num timesteps 2200080, FPS 970, mean/median reward -15.7/-16.0, min/max reward -20.0/-9.0, entropy 1.74181, value loss 0.01275, policy loss -0.01507\n",
      "Updates 27600, num timesteps 2208080, FPS 970, mean/median reward -15.8/-17.0, min/max reward -20.0/-9.0, entropy 1.77228, value loss 0.01119, policy loss 0.01943\n",
      "Updates 27700, num timesteps 2216080, FPS 970, mean/median reward -15.7/-16.0, min/max reward -21.0/-9.0, entropy 1.74459, value loss 0.03911, policy loss -0.04686\n",
      "Updates 27800, num timesteps 2224080, FPS 970, mean/median reward -15.4/-15.0, min/max reward -21.0/-9.0, entropy 1.77981, value loss 0.01001, policy loss -0.03804\n",
      "Updates 27900, num timesteps 2232080, FPS 970, mean/median reward -15.1/-15.0, min/max reward -21.0/-11.0, entropy 1.75306, value loss 0.04037, policy loss 0.01613\n",
      "Updates 28000, num timesteps 2240080, FPS 971, mean/median reward -14.9/-15.0, min/max reward -21.0/-11.0, entropy 1.76851, value loss 0.00733, policy loss -0.01258\n",
      "Updates 28100, num timesteps 2248080, FPS 971, mean/median reward -15.3/-16.0, min/max reward -21.0/-11.0, entropy 1.75757, value loss 0.01275, policy loss -0.04231\n",
      "Updates 28200, num timesteps 2256080, FPS 971, mean/median reward -15.1/-15.0, min/max reward -19.0/-11.0, entropy 1.77016, value loss 0.01661, policy loss 0.03724\n",
      "Updates 28300, num timesteps 2264080, FPS 971, mean/median reward -15.5/-16.0, min/max reward -19.0/-11.0, entropy 1.76953, value loss 0.02473, policy loss 0.06632\n",
      "Updates 28400, num timesteps 2272080, FPS 971, mean/median reward -15.7/-16.0, min/max reward -19.0/-11.0, entropy 1.74534, value loss 0.03458, policy loss -0.04259\n",
      "Updates 28500, num timesteps 2280080, FPS 971, mean/median reward -15.6/-16.0, min/max reward -19.0/-11.0, entropy 1.67520, value loss 0.01649, policy loss 0.02102\n",
      "Updates 28600, num timesteps 2288080, FPS 971, mean/median reward -15.0/-15.0, min/max reward -19.0/-11.0, entropy 1.69029, value loss 0.01210, policy loss 0.06358\n",
      "Updates 28700, num timesteps 2296080, FPS 971, mean/median reward -14.4/-14.0, min/max reward -19.0/-11.0, entropy 1.78253, value loss 0.01009, policy loss -0.04120\n",
      "Updates 28800, num timesteps 2304080, FPS 971, mean/median reward -13.6/-14.0, min/max reward -16.0/-11.0, entropy 1.75798, value loss 0.01770, policy loss 0.08156\n",
      "Updates 28900, num timesteps 2312080, FPS 971, mean/median reward -13.6/-14.0, min/max reward -17.0/-11.0, entropy 1.73387, value loss 0.06452, policy loss -0.06814\n",
      "Updates 29000, num timesteps 2320080, FPS 971, mean/median reward -13.5/-13.0, min/max reward -17.0/-11.0, entropy 1.74363, value loss 0.01320, policy loss 0.04410\n",
      "Updates 29100, num timesteps 2328080, FPS 971, mean/median reward -14.1/-14.0, min/max reward -17.0/-12.0, entropy 1.77989, value loss 0.02544, policy loss 0.00036\n",
      "Updates 29200, num timesteps 2336080, FPS 971, mean/median reward -14.2/-14.0, min/max reward -17.0/-12.0, entropy 1.78048, value loss 0.01200, policy loss 0.04126\n",
      "Updates 29300, num timesteps 2344080, FPS 971, mean/median reward -14.4/-15.0, min/max reward -17.0/-12.0, entropy 1.75130, value loss 0.02168, policy loss -0.06342\n",
      "Updates 29400, num timesteps 2352080, FPS 971, mean/median reward -14.3/-15.0, min/max reward -17.0/-12.0, entropy 1.75499, value loss 0.02780, policy loss -0.07980\n",
      "Updates 29500, num timesteps 2360080, FPS 971, mean/median reward -14.6/-15.0, min/max reward -17.0/-12.0, entropy 1.75591, value loss 0.02694, policy loss -0.05237\n",
      "Updates 29600, num timesteps 2368080, FPS 971, mean/median reward -15.3/-16.0, min/max reward -17.0/-12.0, entropy 1.73711, value loss 0.03747, policy loss -0.12386\n",
      "Updates 29700, num timesteps 2376080, FPS 971, mean/median reward -14.9/-16.0, min/max reward -17.0/-12.0, entropy 1.76314, value loss 0.01892, policy loss 0.03142\n",
      "Updates 29800, num timesteps 2384080, FPS 971, mean/median reward -15.1/-16.0, min/max reward -19.0/-11.0, entropy 1.76943, value loss 0.00808, policy loss 0.02891\n",
      "Updates 29900, num timesteps 2392080, FPS 971, mean/median reward -14.7/-14.0, min/max reward -19.0/-11.0, entropy 1.77612, value loss 0.00474, policy loss 0.00756\n",
      "Updates 30000, num timesteps 2400080, FPS 971, mean/median reward -15.1/-16.0, min/max reward -20.0/-10.0, entropy 1.67792, value loss 0.02860, policy loss 0.04937\n",
      "Updates 30100, num timesteps 2408080, FPS 971, mean/median reward -15.4/-15.0, min/max reward -20.0/-10.0, entropy 1.75630, value loss 0.00837, policy loss -0.03904\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updates 30200, num timesteps 2416080, FPS 971, mean/median reward -15.8/-17.0, min/max reward -20.0/-10.0, entropy 1.77345, value loss 0.01981, policy loss 0.08759\n",
      "Updates 30300, num timesteps 2424080, FPS 971, mean/median reward -15.6/-15.0, min/max reward -20.0/-10.0, entropy 1.77067, value loss 0.01360, policy loss 0.03163\n",
      "Updates 30400, num timesteps 2432080, FPS 971, mean/median reward -16.0/-17.0, min/max reward -20.0/-10.0, entropy 1.75098, value loss 0.00983, policy loss -0.00155\n",
      "Updates 30500, num timesteps 2440080, FPS 971, mean/median reward -15.3/-16.0, min/max reward -19.0/-10.0, entropy 1.76022, value loss 0.02399, policy loss -0.03885\n",
      "Updates 30600, num timesteps 2448080, FPS 971, mean/median reward -14.9/-15.0, min/max reward -19.0/-10.0, entropy 1.75143, value loss 0.03435, policy loss -0.11808\n",
      "Updates 30700, num timesteps 2456080, FPS 971, mean/median reward -15.0/-15.0, min/max reward -19.0/-11.0, entropy 1.71839, value loss 0.02559, policy loss 0.04041\n",
      "Updates 30800, num timesteps 2464080, FPS 971, mean/median reward -14.8/-15.0, min/max reward -18.0/-10.0, entropy 1.74036, value loss 0.06140, policy loss -0.12495\n",
      "Updates 30900, num timesteps 2472080, FPS 971, mean/median reward -14.9/-14.0, min/max reward -19.0/-10.0, entropy 1.76209, value loss 0.05266, policy loss -0.09108\n",
      "Updates 31000, num timesteps 2480080, FPS 971, mean/median reward -15.1/-16.0, min/max reward -19.0/-10.0, entropy 1.77962, value loss 0.00568, policy loss 0.02237\n",
      "Updates 31100, num timesteps 2488080, FPS 971, mean/median reward -15.2/-16.0, min/max reward -19.0/-10.0, entropy 1.75443, value loss 0.02620, policy loss 0.07577\n",
      "Updates 31200, num timesteps 2496080, FPS 971, mean/median reward -14.1/-14.0, min/max reward -19.0/-10.0, entropy 1.75863, value loss 0.00675, policy loss 0.02665\n",
      "Updates 31300, num timesteps 2504080, FPS 971, mean/median reward -13.8/-14.0, min/max reward -19.0/-11.0, entropy 1.73111, value loss 0.05138, policy loss -0.03127\n",
      "Updates 31400, num timesteps 2512080, FPS 971, mean/median reward -13.0/-13.0, min/max reward -17.0/-11.0, entropy 1.77759, value loss 0.02015, policy loss -0.00023\n",
      "Updates 31500, num timesteps 2520080, FPS 972, mean/median reward -13.0/-13.0, min/max reward -19.0/-8.0, entropy 1.76158, value loss 0.01339, policy loss -0.05271\n",
      "Updates 31600, num timesteps 2528080, FPS 972, mean/median reward -14.4/-15.0, min/max reward -19.0/-8.0, entropy 1.74386, value loss 0.04415, policy loss -0.14122\n",
      "Updates 31700, num timesteps 2536080, FPS 972, mean/median reward -14.8/-15.0, min/max reward -19.0/-8.0, entropy 1.73269, value loss 0.01916, policy loss -0.09287\n",
      "Updates 31800, num timesteps 2544080, FPS 972, mean/median reward -15.2/-15.0, min/max reward -19.0/-8.0, entropy 1.78426, value loss 0.00639, policy loss 0.00011\n",
      "Updates 31900, num timesteps 2552080, FPS 972, mean/median reward -15.3/-15.0, min/max reward -19.0/-10.0, entropy 1.78671, value loss 0.08300, policy loss 0.13457\n",
      "Updates 32000, num timesteps 2560080, FPS 972, mean/median reward -15.3/-16.0, min/max reward -19.0/-10.0, entropy 1.78678, value loss 0.01537, policy loss 0.07786\n",
      "Updates 32100, num timesteps 2568080, FPS 972, mean/median reward -15.4/-16.0, min/max reward -19.0/-10.0, entropy 1.73808, value loss 0.02956, policy loss -0.00521\n",
      "Updates 32200, num timesteps 2576080, FPS 972, mean/median reward -15.4/-16.0, min/max reward -19.0/-10.0, entropy 1.76902, value loss 0.03574, policy loss -0.00119\n",
      "Updates 32300, num timesteps 2584080, FPS 972, mean/median reward -15.9/-17.0, min/max reward -19.0/-11.0, entropy 1.77998, value loss 0.01080, policy loss -0.03804\n",
      "Updates 32400, num timesteps 2592080, FPS 972, mean/median reward -15.9/-16.0, min/max reward -19.0/-11.0, entropy 1.74509, value loss 0.01235, policy loss 0.06199\n",
      "Updates 32500, num timesteps 2600080, FPS 972, mean/median reward -15.6/-16.0, min/max reward -19.0/-11.0, entropy 1.76052, value loss 0.04844, policy loss 0.00762\n",
      "Updates 32600, num timesteps 2608080, FPS 972, mean/median reward -15.2/-15.0, min/max reward -17.0/-11.0, entropy 1.78123, value loss 0.03166, policy loss 0.04983\n",
      "Updates 32700, num timesteps 2616080, FPS 972, mean/median reward -15.1/-15.0, min/max reward -17.0/-9.0, entropy 1.77416, value loss 0.01162, policy loss 0.02914\n",
      "Updates 32800, num timesteps 2624080, FPS 972, mean/median reward -14.9/-15.0, min/max reward -17.0/-9.0, entropy 1.75941, value loss 0.03770, policy loss 0.05313\n",
      "Updates 32900, num timesteps 2632080, FPS 972, mean/median reward -14.7/-15.0, min/max reward -18.0/-9.0, entropy 1.76923, value loss 0.00436, policy loss -0.01364\n",
      "Updates 33000, num timesteps 2640080, FPS 972, mean/median reward -14.8/-16.0, min/max reward -18.0/-9.0, entropy 1.73210, value loss 0.01350, policy loss 0.03296\n",
      "Updates 33100, num timesteps 2648080, FPS 972, mean/median reward -14.1/-15.0, min/max reward -18.0/-9.0, entropy 1.78226, value loss 0.02378, policy loss -0.00592\n",
      "Updates 33200, num timesteps 2656080, FPS 972, mean/median reward -13.9/-14.0, min/max reward -18.0/-9.0, entropy 1.74433, value loss 0.02100, policy loss -0.04097\n",
      "Updates 33300, num timesteps 2664080, FPS 972, mean/median reward -13.4/-14.0, min/max reward -17.0/-9.0, entropy 1.77048, value loss 0.03697, policy loss 0.01495\n",
      "Updates 33400, num timesteps 2672080, FPS 972, mean/median reward -13.8/-14.0, min/max reward -18.0/-9.0, entropy 1.77430, value loss 0.00855, policy loss -0.00368\n",
      "Updates 33500, num timesteps 2680080, FPS 972, mean/median reward -14.0/-14.0, min/max reward -18.0/-9.0, entropy 1.74551, value loss 0.03342, policy loss -0.17339\n",
      "Updates 33600, num timesteps 2688080, FPS 972, mean/median reward -14.4/-15.0, min/max reward -19.0/-9.0, entropy 1.76852, value loss 0.01197, policy loss 0.01063\n",
      "Updates 33700, num timesteps 2696080, FPS 972, mean/median reward -14.8/-15.0, min/max reward -19.0/-12.0, entropy 1.78082, value loss 0.01184, policy loss -0.03115\n",
      "Updates 33800, num timesteps 2704080, FPS 972, mean/median reward -15.0/-15.0, min/max reward -19.0/-12.0, entropy 1.74287, value loss 0.02659, policy loss 0.08899\n",
      "Updates 33900, num timesteps 2712080, FPS 972, mean/median reward -15.1/-15.0, min/max reward -19.0/-12.0, entropy 1.75193, value loss 0.01935, policy loss 0.07483\n",
      "Updates 34000, num timesteps 2720080, FPS 972, mean/median reward -14.9/-15.0, min/max reward -19.0/-13.0, entropy 1.73777, value loss 0.05923, policy loss 0.01238\n",
      "Updates 34100, num timesteps 2728080, FPS 972, mean/median reward -14.7/-15.0, min/max reward -19.0/-12.0, entropy 1.74311, value loss 0.05443, policy loss 0.00949\n",
      "Updates 34200, num timesteps 2736080, FPS 972, mean/median reward -14.1/-14.0, min/max reward -19.0/-8.0, entropy 1.66754, value loss 0.02572, policy loss -0.00363\n",
      "Updates 34300, num timesteps 2744080, FPS 972, mean/median reward -14.1/-14.0, min/max reward -19.0/-8.0, entropy 1.76109, value loss 0.01999, policy loss 0.02935\n",
      "Updates 34400, num timesteps 2752080, FPS 972, mean/median reward -13.8/-14.0, min/max reward -18.0/-8.0, entropy 1.76024, value loss 0.03415, policy loss -0.04185\n",
      "Updates 34500, num timesteps 2760080, FPS 972, mean/median reward -13.7/-13.0, min/max reward -18.0/-8.0, entropy 1.72617, value loss 0.02831, policy loss 0.00314\n",
      "Updates 34600, num timesteps 2768080, FPS 972, mean/median reward -13.7/-13.0, min/max reward -18.0/-8.0, entropy 1.78198, value loss 0.01921, policy loss -0.00329\n",
      "Updates 34700, num timesteps 2776080, FPS 972, mean/median reward -12.9/-13.0, min/max reward -18.0/-7.0, entropy 1.77094, value loss 0.01649, policy loss -0.06737\n",
      "Updates 34800, num timesteps 2784080, FPS 972, mean/median reward -11.8/-12.0, min/max reward -18.0/-6.0, entropy 1.78622, value loss 0.00519, policy loss 0.02857\n",
      "Updates 34900, num timesteps 2792080, FPS 972, mean/median reward -11.3/-12.0, min/max reward -17.0/-6.0, entropy 1.72881, value loss 0.03587, policy loss -0.13281\n",
      "Updates 35000, num timesteps 2800080, FPS 972, mean/median reward -10.6/-11.0, min/max reward -15.0/-6.0, entropy 1.71131, value loss 0.04900, policy loss -0.11778\n",
      "Updates 35100, num timesteps 2808080, FPS 972, mean/median reward -11.2/-11.0, min/max reward -18.0/-6.0, entropy 1.77093, value loss 0.03381, policy loss -0.05250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updates 35200, num timesteps 2816080, FPS 972, mean/median reward -12.0/-13.0, min/max reward -18.0/-6.0, entropy 1.76781, value loss 0.01982, policy loss -0.01758\n",
      "Updates 35300, num timesteps 2824080, FPS 972, mean/median reward -12.0/-13.0, min/max reward -18.0/-6.0, entropy 1.73327, value loss 0.01379, policy loss -0.09113\n",
      "Updates 35400, num timesteps 2832080, FPS 972, mean/median reward -12.4/-13.0, min/max reward -18.0/-6.0, entropy 1.76866, value loss 0.01256, policy loss 0.00879\n",
      "Updates 35500, num timesteps 2840080, FPS 972, mean/median reward -12.2/-13.0, min/max reward -16.0/-6.0, entropy 1.74637, value loss 0.02424, policy loss -0.06033\n",
      "Updates 35600, num timesteps 2848080, FPS 972, mean/median reward -12.6/-13.0, min/max reward -16.0/-6.0, entropy 1.77000, value loss 0.02239, policy loss 0.02790\n",
      "Updates 35700, num timesteps 2856080, FPS 972, mean/median reward -12.3/-13.0, min/max reward -17.0/-6.0, entropy 1.66442, value loss 0.06933, policy loss 0.01119\n",
      "Updates 35800, num timesteps 2864080, FPS 972, mean/median reward -11.9/-12.0, min/max reward -17.0/-6.0, entropy 1.71784, value loss 0.04055, policy loss -0.02309\n",
      "Updates 35900, num timesteps 2872080, FPS 972, mean/median reward -12.2/-13.0, min/max reward -17.0/-6.0, entropy 1.78542, value loss 0.00978, policy loss -0.00732\n",
      "Updates 36000, num timesteps 2880080, FPS 972, mean/median reward -12.1/-13.0, min/max reward -17.0/-6.0, entropy 1.73003, value loss 0.02021, policy loss -0.07108\n",
      "Updates 36100, num timesteps 2888080, FPS 972, mean/median reward -12.3/-13.0, min/max reward -17.0/-8.0, entropy 1.76570, value loss 0.01063, policy loss 0.01850\n",
      "Updates 36200, num timesteps 2896080, FPS 972, mean/median reward -12.7/-13.0, min/max reward -17.0/-8.0, entropy 1.70372, value loss 0.04741, policy loss -0.01152\n",
      "Updates 36300, num timesteps 2904080, FPS 972, mean/median reward -12.9/-13.0, min/max reward -17.0/-8.0, entropy 1.75027, value loss 0.02272, policy loss -0.07048\n",
      "Updates 36400, num timesteps 2912080, FPS 972, mean/median reward -12.2/-13.0, min/max reward -16.0/-8.0, entropy 1.77381, value loss 0.01319, policy loss -0.01159\n",
      "Updates 36500, num timesteps 2920080, FPS 972, mean/median reward -11.7/-13.0, min/max reward -16.0/-8.0, entropy 1.76811, value loss 0.02296, policy loss -0.04530\n",
      "Updates 36600, num timesteps 2928080, FPS 972, mean/median reward -12.1/-13.0, min/max reward -16.0/-8.0, entropy 1.77442, value loss 0.01429, policy loss -0.04843\n",
      "Updates 36700, num timesteps 2936080, FPS 972, mean/median reward -12.1/-13.0, min/max reward -16.0/-8.0, entropy 1.73837, value loss 0.02961, policy loss 0.06346\n",
      "Updates 36800, num timesteps 2944080, FPS 972, mean/median reward -12.6/-13.0, min/max reward -17.0/-8.0, entropy 1.73958, value loss 0.02032, policy loss -0.03022\n",
      "Updates 36900, num timesteps 2952080, FPS 972, mean/median reward -12.5/-13.0, min/max reward -17.0/-8.0, entropy 1.75075, value loss 0.03821, policy loss 0.06167\n",
      "Updates 37000, num timesteps 2960080, FPS 972, mean/median reward -11.7/-12.0, min/max reward -17.0/-5.0, entropy 1.75727, value loss 0.05072, policy loss -0.03772\n",
      "Updates 37100, num timesteps 2968080, FPS 972, mean/median reward -11.6/-12.0, min/max reward -17.0/-5.0, entropy 1.76079, value loss 0.02984, policy loss -0.04716\n",
      "Updates 37200, num timesteps 2976080, FPS 972, mean/median reward -10.6/-11.0, min/max reward -14.0/-5.0, entropy 1.76913, value loss 0.01481, policy loss -0.00066\n",
      "Updates 37300, num timesteps 2984080, FPS 972, mean/median reward -10.4/-11.0, min/max reward -14.0/-5.0, entropy 1.76713, value loss 0.03851, policy loss 0.03046\n",
      "Updates 37400, num timesteps 2992080, FPS 973, mean/median reward -10.6/-11.0, min/max reward -17.0/-5.0, entropy 1.75668, value loss 0.01134, policy loss -0.02785\n",
      "Updates 37500, num timesteps 3000080, FPS 973, mean/median reward -10.6/-10.0, min/max reward -17.0/-6.0, entropy 1.74724, value loss 0.00964, policy loss 0.00908\n",
      "Updates 37600, num timesteps 3008080, FPS 973, mean/median reward -10.9/-12.0, min/max reward -17.0/-6.0, entropy 1.77416, value loss 0.02376, policy loss -0.02022\n",
      "Updates 37700, num timesteps 3016080, FPS 973, mean/median reward -10.6/-10.0, min/max reward -17.0/-6.0, entropy 1.78664, value loss 0.00401, policy loss 0.01159\n",
      "Updates 37800, num timesteps 3024080, FPS 973, mean/median reward -10.0/-10.0, min/max reward -14.0/-6.0, entropy 1.75764, value loss 0.02709, policy loss -0.00528\n",
      "Updates 37900, num timesteps 3032080, FPS 973, mean/median reward -10.8/-11.0, min/max reward -14.0/-6.0, entropy 1.74618, value loss 0.01161, policy loss -0.02942\n",
      "Updates 38000, num timesteps 3040080, FPS 973, mean/median reward -10.6/-11.0, min/max reward -14.0/-6.0, entropy 1.71265, value loss 0.02718, policy loss -0.07185\n",
      "Updates 38100, num timesteps 3048080, FPS 973, mean/median reward -11.6/-12.0, min/max reward -18.0/-6.0, entropy 1.74664, value loss 0.02867, policy loss -0.10093\n",
      "Updates 38200, num timesteps 3056080, FPS 973, mean/median reward -11.6/-12.0, min/max reward -18.0/-6.0, entropy 1.72502, value loss 0.02988, policy loss -0.11236\n",
      "Updates 38300, num timesteps 3064080, FPS 973, mean/median reward -11.1/-11.0, min/max reward -18.0/-6.0, entropy 1.75070, value loss 0.02782, policy loss -0.11703\n",
      "Updates 38400, num timesteps 3072080, FPS 973, mean/median reward -10.3/-10.0, min/max reward -18.0/-6.0, entropy 1.76986, value loss 0.02936, policy loss -0.06019\n",
      "Updates 38500, num timesteps 3080080, FPS 973, mean/median reward -9.9/-10.0, min/max reward -14.0/-6.0, entropy 1.73920, value loss 0.00629, policy loss -0.03166\n",
      "Updates 38600, num timesteps 3088080, FPS 973, mean/median reward -10.6/-10.0, min/max reward -17.0/-6.0, entropy 1.78927, value loss 0.00536, policy loss 0.00377\n",
      "Updates 38700, num timesteps 3096080, FPS 973, mean/median reward -10.8/-10.0, min/max reward -17.0/-6.0, entropy 1.72068, value loss 0.02189, policy loss 0.02125\n",
      "Updates 38800, num timesteps 3104080, FPS 973, mean/median reward -10.9/-11.0, min/max reward -17.0/-6.0, entropy 1.72158, value loss 0.03370, policy loss -0.05340\n",
      "Updates 38900, num timesteps 3112080, FPS 973, mean/median reward -10.3/-10.0, min/max reward -17.0/-6.0, entropy 1.77400, value loss 0.05916, policy loss 0.02073\n",
      "Updates 39000, num timesteps 3120080, FPS 973, mean/median reward -10.4/-10.0, min/max reward -17.0/-6.0, entropy 1.77444, value loss 0.06131, policy loss 0.07221\n",
      "Updates 39100, num timesteps 3128080, FPS 973, mean/median reward -9.9/-10.0, min/max reward -17.0/-6.0, entropy 1.73635, value loss 0.02209, policy loss 0.03759\n",
      "Updates 39200, num timesteps 3136080, FPS 973, mean/median reward -8.5/-8.0, min/max reward -13.0/-6.0, entropy 1.71591, value loss 0.05487, policy loss -0.00653\n",
      "Updates 39300, num timesteps 3144080, FPS 973, mean/median reward -7.9/-7.0, min/max reward -12.0/-6.0, entropy 1.69583, value loss 0.01240, policy loss 0.00902\n",
      "Updates 39400, num timesteps 3152080, FPS 973, mean/median reward -8.2/-8.0, min/max reward -13.0/-6.0, entropy 1.74282, value loss 0.00513, policy loss -0.01381\n",
      "Updates 39500, num timesteps 3160080, FPS 973, mean/median reward -8.2/-8.0, min/max reward -13.0/-4.0, entropy 1.76755, value loss 0.02807, policy loss -0.04326\n",
      "Updates 39600, num timesteps 3168080, FPS 973, mean/median reward -8.6/-9.0, min/max reward -13.0/-4.0, entropy 1.77696, value loss 0.02615, policy loss -0.02516\n",
      "Updates 39700, num timesteps 3176080, FPS 973, mean/median reward -8.8/-9.0, min/max reward -13.0/-1.0, entropy 1.77922, value loss 0.00848, policy loss 0.02961\n",
      "Updates 39800, num timesteps 3184080, FPS 973, mean/median reward -8.9/-9.0, min/max reward -13.0/-1.0, entropy 1.77374, value loss 0.02079, policy loss 0.04259\n",
      "Updates 39900, num timesteps 3192080, FPS 973, mean/median reward -8.1/-9.0, min/max reward -13.0/-1.0, entropy 1.78081, value loss 0.05349, policy loss 0.07754\n",
      "Updates 40000, num timesteps 3200080, FPS 973, mean/median reward -7.8/-9.0, min/max reward -13.0/-1.0, entropy 1.75576, value loss 0.00974, policy loss -0.03966\n",
      "Updates 40100, num timesteps 3208080, FPS 973, mean/median reward -7.8/-9.0, min/max reward -13.0/-1.0, entropy 1.73854, value loss 0.07587, policy loss 0.12359\n",
      "Updates 40200, num timesteps 3216080, FPS 973, mean/median reward -6.2/-8.0, min/max reward -11.0/4.0, entropy 1.77522, value loss 0.06721, policy loss 0.07100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updates 40300, num timesteps 3224080, FPS 973, mean/median reward -6.6/-8.0, min/max reward -14.0/4.0, entropy 1.69997, value loss 0.01959, policy loss 0.06930\n",
      "Updates 40400, num timesteps 3232080, FPS 973, mean/median reward -6.9/-8.0, min/max reward -14.0/4.0, entropy 1.77823, value loss 0.01306, policy loss -0.00873\n",
      "Updates 40500, num timesteps 3240080, FPS 973, mean/median reward -7.2/-9.0, min/max reward -14.0/4.0, entropy 1.77146, value loss 0.05634, policy loss 0.00381\n",
      "Updates 40600, num timesteps 3248080, FPS 973, mean/median reward -7.2/-9.0, min/max reward -14.0/4.0, entropy 1.64191, value loss 0.01695, policy loss -0.03295\n",
      "Updates 40700, num timesteps 3256080, FPS 973, mean/median reward -7.4/-9.0, min/max reward -14.0/4.0, entropy 1.77805, value loss 0.02736, policy loss 0.10404\n",
      "Updates 40800, num timesteps 3264080, FPS 973, mean/median reward -8.4/-9.0, min/max reward -14.0/-2.0, entropy 1.72492, value loss 0.02657, policy loss 0.05143\n",
      "Updates 40900, num timesteps 3272080, FPS 973, mean/median reward -8.4/-9.0, min/max reward -13.0/-2.0, entropy 1.76842, value loss 0.06392, policy loss 0.10333\n",
      "Updates 41000, num timesteps 3280080, FPS 973, mean/median reward -7.8/-9.0, min/max reward -13.0/-2.0, entropy 1.75364, value loss 0.01868, policy loss -0.06744\n",
      "Updates 41100, num timesteps 3288080, FPS 973, mean/median reward -7.7/-7.0, min/max reward -13.0/-2.0, entropy 1.75249, value loss 0.08969, policy loss -0.09617\n",
      "Updates 41200, num timesteps 3296080, FPS 973, mean/median reward -7.9/-9.0, min/max reward -13.0/-2.0, entropy 1.78192, value loss 0.02666, policy loss 0.00383\n",
      "Updates 41300, num timesteps 3304080, FPS 973, mean/median reward -8.1/-9.0, min/max reward -13.0/-2.0, entropy 1.74018, value loss 0.04557, policy loss 0.02663\n",
      "Updates 41400, num timesteps 3312080, FPS 973, mean/median reward -7.6/-8.0, min/max reward -13.0/-2.0, entropy 1.71656, value loss 0.05186, policy loss -0.16837\n",
      "Updates 41500, num timesteps 3320080, FPS 973, mean/median reward -7.6/-8.0, min/max reward -13.0/-3.0, entropy 1.73704, value loss 0.02447, policy loss -0.03554\n",
      "Updates 41600, num timesteps 3328080, FPS 973, mean/median reward -7.8/-8.0, min/max reward -14.0/-3.0, entropy 1.73310, value loss 0.03147, policy loss -0.03665\n",
      "Updates 41700, num timesteps 3336080, FPS 973, mean/median reward -7.7/-8.0, min/max reward -14.0/-3.0, entropy 1.76035, value loss 0.01556, policy loss 0.00093\n",
      "Updates 41800, num timesteps 3344080, FPS 973, mean/median reward -7.7/-8.0, min/max reward -14.0/-3.0, entropy 1.75552, value loss 0.00909, policy loss 0.02680\n",
      "Updates 41900, num timesteps 3352080, FPS 973, mean/median reward -6.6/-7.0, min/max reward -14.0/-2.0, entropy 1.77186, value loss 0.01240, policy loss -0.07905\n",
      "Updates 42000, num timesteps 3360080, FPS 973, mean/median reward -6.0/-6.0, min/max reward -14.0/-2.0, entropy 1.73354, value loss 0.01028, policy loss 0.02890\n",
      "Updates 42100, num timesteps 3368080, FPS 973, mean/median reward -5.8/-6.0, min/max reward -14.0/-2.0, entropy 1.76304, value loss 0.02577, policy loss 0.01826\n",
      "Updates 42200, num timesteps 3376080, FPS 973, mean/median reward -5.2/-5.0, min/max reward -8.0/-2.0, entropy 1.75933, value loss 0.01130, policy loss 0.02932\n",
      "Updates 42300, num timesteps 3384080, FPS 973, mean/median reward -4.9/-5.0, min/max reward -8.0/-2.0, entropy 1.74483, value loss 0.01984, policy loss -0.03339\n",
      "Updates 42400, num timesteps 3392080, FPS 973, mean/median reward -4.9/-5.0, min/max reward -8.0/-2.0, entropy 1.75275, value loss 0.00982, policy loss 0.00539\n",
      "Updates 42500, num timesteps 3400080, FPS 973, mean/median reward -5.8/-6.0, min/max reward -9.0/-2.0, entropy 1.72351, value loss 0.01253, policy loss 0.06726\n",
      "Updates 42600, num timesteps 3408080, FPS 973, mean/median reward -5.9/-6.0, min/max reward -10.0/-3.0, entropy 1.68335, value loss 0.03085, policy loss 0.04060\n",
      "Updates 42700, num timesteps 3416080, FPS 973, mean/median reward -6.0/-6.0, min/max reward -10.0/-3.0, entropy 1.73949, value loss 0.00657, policy loss -0.00311\n",
      "Updates 42800, num timesteps 3424080, FPS 973, mean/median reward -6.1/-7.0, min/max reward -10.0/-1.0, entropy 1.74723, value loss 0.03299, policy loss -0.00048\n",
      "Updates 42900, num timesteps 3432080, FPS 973, mean/median reward -6.3/-8.0, min/max reward -10.0/-1.0, entropy 1.73210, value loss 0.03456, policy loss 0.02348\n",
      "Updates 43000, num timesteps 3440080, FPS 973, mean/median reward -6.8/-8.0, min/max reward -13.0/-1.0, entropy 1.77059, value loss 0.02897, policy loss -0.02524\n",
      "Updates 43100, num timesteps 3448080, FPS 973, mean/median reward -6.9/-8.0, min/max reward -13.0/-1.0, entropy 1.77127, value loss 0.02607, policy loss 0.05583\n",
      "Updates 43200, num timesteps 3456080, FPS 973, mean/median reward -6.9/-8.0, min/max reward -13.0/-1.0, entropy 1.75325, value loss 0.02289, policy loss -0.08073\n",
      "Updates 43300, num timesteps 3464080, FPS 973, mean/median reward -6.7/-8.0, min/max reward -13.0/-1.0, entropy 1.68429, value loss 0.03913, policy loss 0.05010\n",
      "Updates 43400, num timesteps 3472080, FPS 973, mean/median reward -6.7/-7.0, min/max reward -13.0/1.0, entropy 1.72081, value loss 0.02898, policy loss -0.02320\n",
      "Updates 43500, num timesteps 3480080, FPS 973, mean/median reward -6.6/-7.0, min/max reward -12.0/1.0, entropy 1.75477, value loss 0.04193, policy loss -0.08454\n",
      "Updates 43600, num timesteps 3488080, FPS 973, mean/median reward -7.0/-8.0, min/max reward -12.0/1.0, entropy 1.74675, value loss 0.02527, policy loss -0.00965\n",
      "Updates 43700, num timesteps 3496080, FPS 973, mean/median reward -6.4/-7.0, min/max reward -10.0/1.0, entropy 1.73545, value loss 0.04526, policy loss -0.11950\n",
      "Updates 43800, num timesteps 3504080, FPS 973, mean/median reward -6.4/-7.0, min/max reward -10.0/1.0, entropy 1.78621, value loss 0.07646, policy loss -0.13388\n",
      "Updates 43900, num timesteps 3512080, FPS 973, mean/median reward -6.8/-8.0, min/max reward -10.0/1.0, entropy 1.75546, value loss 0.02269, policy loss -0.01932\n",
      "Updates 44000, num timesteps 3520080, FPS 973, mean/median reward -7.7/-9.0, min/max reward -12.0/-4.0, entropy 1.64419, value loss 0.02941, policy loss -0.05704\n",
      "Updates 44100, num timesteps 3528080, FPS 973, mean/median reward -7.2/-8.0, min/max reward -12.0/-3.0, entropy 1.75794, value loss 0.02875, policy loss -0.09091\n",
      "Updates 44200, num timesteps 3536080, FPS 973, mean/median reward -7.0/-8.0, min/max reward -12.0/-3.0, entropy 1.74301, value loss 0.00810, policy loss -0.01876\n",
      "Updates 44300, num timesteps 3544080, FPS 973, mean/median reward -6.6/-6.0, min/max reward -12.0/-3.0, entropy 1.75047, value loss 0.02055, policy loss 0.05938\n",
      "Updates 44400, num timesteps 3552080, FPS 973, mean/median reward -6.1/-6.0, min/max reward -12.0/-1.0, entropy 1.71189, value loss 0.04958, policy loss -0.02919\n",
      "Updates 44500, num timesteps 3560080, FPS 973, mean/median reward -6.2/-7.0, min/max reward -10.0/-1.0, entropy 1.76598, value loss 0.03957, policy loss -0.09357\n",
      "Updates 44600, num timesteps 3568080, FPS 974, mean/median reward -6.9/-8.0, min/max reward -11.0/-1.0, entropy 1.73745, value loss 0.01011, policy loss -0.03542\n",
      "Updates 44700, num timesteps 3576080, FPS 974, mean/median reward -7.5/-8.0, min/max reward -11.0/-1.0, entropy 1.68621, value loss 0.03325, policy loss 0.04053\n",
      "Updates 44800, num timesteps 3584080, FPS 974, mean/median reward -7.1/-8.0, min/max reward -11.0/-1.0, entropy 1.71516, value loss 0.02210, policy loss -0.06560\n",
      "Updates 44900, num timesteps 3592080, FPS 974, mean/median reward -7.6/-8.0, min/max reward -12.0/-3.0, entropy 1.78025, value loss 0.00715, policy loss -0.06493\n",
      "Updates 45000, num timesteps 3600080, FPS 974, mean/median reward -7.6/-8.0, min/max reward -12.0/-3.0, entropy 1.71324, value loss 0.01630, policy loss -0.06161\n",
      "Updates 45100, num timesteps 3608080, FPS 974, mean/median reward -7.4/-8.0, min/max reward -12.0/-3.0, entropy 1.75056, value loss 0.01247, policy loss -0.01686\n",
      "Updates 45200, num timesteps 3616080, FPS 974, mean/median reward -6.8/-7.0, min/max reward -12.0/-3.0, entropy 1.77404, value loss 0.01136, policy loss 0.04380\n",
      "Updates 45300, num timesteps 3624080, FPS 974, mean/median reward -6.6/-6.0, min/max reward -12.0/-3.0, entropy 1.76180, value loss 0.02163, policy loss -0.06658\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updates 45400, num timesteps 3632080, FPS 974, mean/median reward -6.4/-6.0, min/max reward -12.0/-3.0, entropy 1.77631, value loss 0.07451, policy loss 0.10090\n",
      "Updates 45500, num timesteps 3640080, FPS 974, mean/median reward -6.6/-6.0, min/max reward -12.0/-3.0, entropy 1.77519, value loss 0.01599, policy loss -0.09302\n",
      "Updates 45600, num timesteps 3648080, FPS 974, mean/median reward -5.6/-6.0, min/max reward -10.0/2.0, entropy 1.71904, value loss 0.05460, policy loss -0.03105\n",
      "Updates 45700, num timesteps 3656080, FPS 974, mean/median reward -4.9/-5.0, min/max reward -10.0/2.0, entropy 1.75295, value loss 0.02363, policy loss 0.01933\n",
      "Updates 45800, num timesteps 3664080, FPS 974, mean/median reward -4.6/-5.0, min/max reward -10.0/2.0, entropy 1.73894, value loss 0.01364, policy loss 0.09037\n",
      "Updates 45900, num timesteps 3672080, FPS 974, mean/median reward -4.6/-5.0, min/max reward -10.0/2.0, entropy 1.70345, value loss 0.06189, policy loss 0.15229\n",
      "Updates 46000, num timesteps 3680080, FPS 974, mean/median reward -4.8/-5.0, min/max reward -12.0/2.0, entropy 1.76321, value loss 0.01263, policy loss 0.08153\n",
      "Updates 46100, num timesteps 3688080, FPS 974, mean/median reward -5.1/-5.0, min/max reward -12.0/2.0, entropy 1.77043, value loss 0.02053, policy loss -0.02826\n",
      "Updates 46200, num timesteps 3696080, FPS 974, mean/median reward -5.5/-5.0, min/max reward -12.0/-1.0, entropy 1.71683, value loss 0.01888, policy loss 0.05662\n",
      "Updates 46300, num timesteps 3704080, FPS 974, mean/median reward -5.9/-5.0, min/max reward -12.0/-1.0, entropy 1.72007, value loss 0.01293, policy loss -0.01039\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process Process-9:\n",
      "Process Process-10:\n",
      "Process Process-8:\n",
      "Process Process-2:\n",
      "Process Process-1:\n",
      "Process Process-4:\n",
      "Process Process-11:\n",
      "Process Process-7:\n",
      "Process Process-5:\n",
      "Process Process-6:\n",
      "Process Process-15:\n",
      "Process Process-3:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Process Process-12:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "Process Process-16:\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-6-81cd713ae1b0>\", line 7, in worker\n",
      "    cmd, data = remote.recv()\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "Process Process-14:\n",
      "Process Process-13:\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"<ipython-input-6-81cd713ae1b0>\", line 7, in worker\n",
      "    cmd, data = remote.recv()\n",
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-6-81cd713ae1b0>\", line 7, in worker\n",
      "    cmd, data = remote.recv()\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"<ipython-input-6-81cd713ae1b0>\", line 7, in worker\n",
      "    cmd, data = remote.recv()\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 250, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"<ipython-input-6-81cd713ae1b0>\", line 7, in worker\n",
      "    cmd, data = remote.recv()\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 250, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"<ipython-input-6-81cd713ae1b0>\", line 7, in worker\n",
      "    cmd, data = remote.recv()\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 250, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 250, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 250, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"<ipython-input-6-81cd713ae1b0>\", line 7, in worker\n",
      "    cmd, data = remote.recv()\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 250, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"<ipython-input-6-81cd713ae1b0>\", line 9, in worker\n",
      "    ob, reward, done, info = env.step(data)\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"<ipython-input-6-81cd713ae1b0>\", line 7, in worker\n",
      "    cmd, data = remote.recv()\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 250, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"<ipython-input-6-81cd713ae1b0>\", line 7, in worker\n",
      "    cmd, data = remote.recv()\n",
      "  File \"<ipython-input-6-81cd713ae1b0>\", line 9, in worker\n",
      "    ob, reward, done, info = env.step(data)\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "KeyboardInterrupt\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "  File \"<ipython-input-6-81cd713ae1b0>\", line 7, in worker\n",
      "    cmd, data = remote.recv()\n",
      "  File \"<ipython-input-6-81cd713ae1b0>\", line 7, in worker\n",
      "    cmd, data = remote.recv()\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/site-packages/gym/core.py\", line 96, in step\n",
      "    return self._step(action)\n",
      "  File \"<ipython-input-6-81cd713ae1b0>\", line 7, in worker\n",
      "    cmd, data = remote.recv()\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 250, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "  File \"<ipython-input-6-81cd713ae1b0>\", line 9, in worker\n",
      "    ob, reward, done, info = env.step(data)\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 250, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 250, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/site-packages/gym/core.py\", line 315, in _step\n",
      "    observation, reward, done, info = self.env.step(action)\n",
      "KeyboardInterrupt\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "KeyboardInterrupt\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 250, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/site-packages/gym/core.py\", line 96, in step\n",
      "    return self._step(action)\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/site-packages/gym/core.py\", line 96, in step\n",
      "    return self._step(action)\n",
      "  File \"<ipython-input-6-81cd713ae1b0>\", line 7, in worker\n",
      "    cmd, data = remote.recv()\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "KeyboardInterrupt\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/site-packages/gym/core.py\", line 96, in step\n",
      "    return self._step(action)\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 250, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/site-packages/gym/core.py\", line 315, in _step\n",
      "    observation, reward, done, info = self.env.step(action)\n",
      "KeyboardInterrupt\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 250, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/site-packages/gym/core.py\", line 315, in _step\n",
      "    observation, reward, done, info = self.env.step(action)\n",
      "KeyboardInterrupt\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/site-packages/gym/core.py\", line 326, in _step\n",
      "    observation, reward, done, info = self.env.step(action)\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/site-packages/gym/core.py\", line 96, in step\n",
      "    return self._step(action)\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/site-packages/gym/core.py\", line 96, in step\n",
      "    return self._step(action)\n",
      "KeyboardInterrupt\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/site-packages/gym/core.py\", line 326, in _step\n",
      "    observation, reward, done, info = self.env.step(action)\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/site-packages/gym/core.py\", line 315, in _step\n",
      "    observation, reward, done, info = self.env.step(action)\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/site-packages/gym/core.py\", line 96, in step\n",
      "    return self._step(action)\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/site-packages/gym/core.py\", line 96, in step\n",
      "    return self._step(action)\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/site-packages/gym/core.py\", line 315, in _step\n",
      "    observation, reward, done, info = self.env.step(action)\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/site-packages/gym/core.py\", line 280, in _step\n",
      "    return self.env.step(action)\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/site-packages/gym/core.py\", line 96, in step\n",
      "    return self._step(action)\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/site-packages/gym/core.py\", line 96, in step\n",
      "    return self._step(action)\n",
      "KeyboardInterrupt\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/site-packages/gym/core.py\", line 96, in step\n",
      "    return self._step(action)\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/site-packages/gym/core.py\", line 326, in _step\n",
      "    observation, reward, done, info = self.env.step(action)\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/site-packages/gym/core.py\", line 280, in _step\n",
      "    return self.env.step(action)\n",
      "  File \"/home/ubuntu/baselines-A2C/atari_wrappers.py\", line 60, in _step\n",
      "    obs, reward, done, info = self.env.step(action)\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/site-packages/gym/core.py\", line 96, in step\n",
      "    return self._step(action)\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/site-packages/gym/core.py\", line 96, in step\n",
      "    return self._step(action)\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/site-packages/gym/core.py\", line 96, in step\n",
      "    return self._step(action)\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/site-packages/gym/core.py\", line 315, in _step\n",
      "    observation, reward, done, info = self.env.step(action)\n",
      "  File \"/home/ubuntu/baselines-A2C/atari_wrappers.py\", line 60, in _step\n",
      "    obs, reward, done, info = self.env.step(action)\n",
      "KeyboardInterrupt\n",
      "  File \"/home/ubuntu/baselines-A2C/atari_wrappers.py\", line 99, in _step\n",
      "    obs, reward, done, info = self.env.step(action)\n",
      "KeyboardInterrupt\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/site-packages/gym/core.py\", line 96, in step\n",
      "    return self._step(action)\n",
      "KeyboardInterrupt\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/site-packages/gym/core.py\", line 96, in step\n",
      "    return self._step(action)\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/site-packages/gym/core.py\", line 96, in step\n",
      "    return self._step(action)\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/site-packages/gym/core.py\", line 280, in _step\n",
      "    return self.env.step(action)\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/site-packages/gym/core.py\", line 280, in _step\n",
      "    return self.env.step(action)\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/site-packages/gym/core.py\", line 96, in step\n",
      "    return self._step(action)\n",
      "  File \"/home/ubuntu/baselines-A2C/atari_wrappers.py\", line 60, in _step\n",
      "    obs, reward, done, info = self.env.step(action)\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/site-packages/gym/core.py\", line 96, in step\n",
      "    return self._step(action)\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/site-packages/gym/core.py\", line 96, in step\n",
      "    return self._step(action)\n",
      "  File \"/home/ubuntu/baselines-A2C/atari_wrappers.py\", line 99, in _step\n",
      "    obs, reward, done, info = self.env.step(action)\n",
      "  File \"/home/ubuntu/baselines-A2C/atari_wrappers.py\", line 99, in _step\n",
      "    obs, reward, done, info = self.env.step(action)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/site-packages/gym/wrappers/time_limit.py\", line 36, in _step\n",
      "    observation, reward, done, info = self.env.step(action)\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/site-packages/gym/core.py\", line 96, in step\n",
      "    return self._step(action)\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/site-packages/gym/core.py\", line 96, in step\n",
      "    return self._step(action)\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/site-packages/gym/core.py\", line 96, in step\n",
      "    return self._step(action)\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/site-packages/gym/core.py\", line 280, in _step\n",
      "    return self.env.step(action)\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/site-packages/gym/envs/atari/atari_env.py\", line 80, in _step\n",
      "    reward += self.ale.act(action)\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/site-packages/gym/core.py\", line 280, in _step\n",
      "    return self.env.step(action)\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/site-packages/atari_py/ale_python_interface.py\", line 136, in act\n",
      "    return ale_lib.act(self.obj, int(action))\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/site-packages/gym/core.py\", line 96, in step\n",
      "    return self._step(action)\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/site-packages/gym/wrappers/time_limit.py\", line 36, in _step\n",
      "    observation, reward, done, info = self.env.step(action)\n",
      "KeyboardInterrupt\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/site-packages/gym/core.py\", line 96, in step\n",
      "    return self._step(action)\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/site-packages/gym/core.py\", line 96, in step\n",
      "    return self._step(action)\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/site-packages/gym/envs/atari/atari_env.py\", line 80, in _step\n",
      "    reward += self.ale.act(action)\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/site-packages/atari_py/ale_python_interface.py\", line 136, in act\n",
      "    return ale_lib.act(self.obj, int(action))\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/site-packages/gym/wrappers/time_limit.py\", line 36, in _step\n",
      "    observation, reward, done, info = self.env.step(action)\n",
      "KeyboardInterrupt\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/site-packages/gym/core.py\", line 96, in step\n",
      "    return self._step(action)\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/site-packages/gym/envs/atari/atari_env.py\", line 81, in _step\n",
      "    ob = self._get_obs()\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/site-packages/gym/envs/atari/atari_env.py\", line 99, in _get_obs\n",
      "    img = self._get_image()\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/site-packages/gym/envs/atari/atari_env.py\", line 86, in _get_image\n",
      "    return self.ale.getScreenRGB2()\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/site-packages/atari_py/ale_python_interface.py\", line 221, in getScreenRGB2\n",
      "    ale_lib.getScreenRGB2(self.obj, as_ctypes(screen_data[:]))\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-ae56f0edf401>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    125\u001b[0m                        value_loss.data[0], action_loss.data[0]))\n\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-7-ae56f0edf401>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     57\u001b[0m             \u001b[0mcpu_actions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m             \u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menvs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcpu_actions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m             \u001b[0mreward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreward\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0mepisode_rewards\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-81cd713ae1b0>\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, actions)\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mremote\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mremotes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             \u001b[0mremote\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'step'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mremote\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mremote\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mremotes\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrews\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdones\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/multiprocessing/connection.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    204\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_closed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_writable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 206\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_send_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ForkingPickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdumps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    207\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrecv_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxlength\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/multiprocessing/connection.py\u001b[0m in \u001b[0;36m_send_bytes\u001b[0;34m(self, buf)\u001b[0m\n\u001b[1;32m    402\u001b[0m             \u001b[0;31m# Also note we want to avoid sending a 0-length buffer separately,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    403\u001b[0m             \u001b[0;31m# to avoid \"broken pipe\" errors if the other end closed the pipe.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 404\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_send\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mheader\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mbuf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    405\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    406\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_recv_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/multiprocessing/connection.py\u001b[0m in \u001b[0;36m_send\u001b[0;34m(self, buf, write)\u001b[0m\n\u001b[1;32m    366\u001b[0m         \u001b[0mremaining\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    367\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 368\u001b[0;31m             \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    369\u001b[0m             \u001b[0mremaining\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    370\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mremaining\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    os.environ['OMP_NUM_THREADS'] = '1'\n",
    "\n",
    "    envs = [make_env(args.env_name, args.seed, i, args.log_dir)\n",
    "                for i in range(args.num_processes)]\n",
    "\n",
    "    if args.num_processes > 1:\n",
    "        envs = SubprocVecEnv(envs)\n",
    "    else:\n",
    "        envs = DummyVecEnv(envs)\n",
    "\n",
    "    obs_shape = envs.observation_space.shape\n",
    "    obs_shape = (obs_shape[0] * args.num_stack, *obs_shape[1:])\n",
    "    \n",
    "    actor_critic = CNNPolicy(obs_shape[0], envs.action_space)\n",
    "  \n",
    "    if args.load_model:\n",
    "        actor_critic.load_state_dict(torch.load(LOAD_PATH))\n",
    "\n",
    "    action_shape = 1\n",
    "\n",
    "    if args.cuda:\n",
    "        actor_critic.cuda()\n",
    "\n",
    "    optimizer = optim.RMSprop(actor_critic.parameters(), args.lr, eps=args.eps, alpha=args.alpha)\n",
    "\n",
    "    rollouts = RolloutStorage(args.num_steps, args.num_processes, obs_shape, envs.action_space,\\\n",
    "                              actor_critic.state_size)\n",
    "    \n",
    "    current_obs = torch.zeros(args.num_processes, *obs_shape)\n",
    "\n",
    "    def update_current_obs(obs):\n",
    "        shape_dim0 = envs.observation_space.shape[0]\n",
    "        obs = torch.from_numpy(obs).float()\n",
    "        if args.num_stack > 1:\n",
    "            current_obs[:, :-shape_dim0] = current_obs[:, shape_dim0:]\n",
    "        current_obs[:, -shape_dim0:] = obs\n",
    "\n",
    "    obs = envs.reset()\n",
    "    update_current_obs(obs)\n",
    "\n",
    "    rollouts.observations[0].copy_(current_obs)\n",
    "    \n",
    "    episode_rewards = torch.zeros([args.num_processes, 1])\n",
    "    final_rewards = torch.zeros([args.num_processes, 1])\n",
    "\n",
    "    if args.cuda:\n",
    "        current_obs = current_obs.cuda()\n",
    "        rollouts.cuda()\n",
    "\n",
    "    start = time.time()\n",
    "    for j in range(num_updates):\n",
    "        for step in range(args.num_steps):\n",
    "            \n",
    "            value, action, action_log_prob, states = actor_critic.act(Variable(rollouts.observations[step], volatile=True),\n",
    "                                                                      Variable(rollouts.states[step], volatile=True),\n",
    "                                                                      Variable(rollouts.masks[step], volatile=True))\n",
    "            cpu_actions = action.data.squeeze(1).cpu().numpy()\n",
    "\n",
    "            obs, reward, done, info = envs.step(cpu_actions)\n",
    "            reward = torch.from_numpy(np.expand_dims(np.stack(reward), 1)).float()\n",
    "            episode_rewards += reward\n",
    "\n",
    "            # If done then clean the history of observations.\n",
    "            masks = torch.FloatTensor([[0.0] if done_ else [1.0] for done_ in done])\n",
    "            final_rewards *= masks\n",
    "            final_rewards += (1 - masks) * episode_rewards\n",
    "            episode_rewards *= masks\n",
    "\n",
    "            if args.cuda:\n",
    "                masks = masks.cuda()\n",
    "\n",
    "            if current_obs.dim() == 4:\n",
    "                current_obs *= masks.unsqueeze(2).unsqueeze(2)\n",
    "            else:\n",
    "                current_obs *= masks\n",
    "\n",
    "            update_current_obs(obs)\n",
    "            rollouts.insert(step, current_obs, states.data, action.data, action_log_prob.data, value.data, reward, masks)\n",
    "\n",
    "        next_value = actor_critic(Variable(rollouts.observations[-1], volatile=True),\n",
    "                                  Variable(rollouts.states[-1], volatile=True),\n",
    "                                  Variable(rollouts.masks[-1], volatile=True))[0].data\n",
    "\n",
    "        rollouts.compute_returns(next_value, args.use_gae, args.gamma, args.tau)\n",
    "\n",
    "        values, action_log_probs, dist_entropy, states = actor_critic.evaluate_actions(Variable(rollouts.observations[:-1].view(-1, *obs_shape)),\n",
    "                                                                                       Variable(rollouts.states[0].view(-1, actor_critic.state_size)),\n",
    "                                                                                       Variable(rollouts.masks[:-1].view(-1, 1)),\n",
    "                                                                                       Variable(rollouts.actions.view(-1, action_shape)))\n",
    "\n",
    "        values = values.view(args.num_steps, args.num_processes, 1)\n",
    "        action_log_probs = action_log_probs.view(args.num_steps, args.num_processes, 1)\n",
    "\n",
    "        advantages = Variable(rollouts.returns[:-1]) - values\n",
    "        value_loss = advantages.pow(2).mean()\n",
    "\n",
    "        action_loss = -(Variable(advantages.data) * action_log_probs).mean()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        (value_loss * args.value_loss_coef + action_loss - dist_entropy * args.entropy_coef).backward()\n",
    "\n",
    "        nn.utils.clip_grad_norm(actor_critic.parameters(), args.max_grad_norm)\n",
    "\n",
    "        optimizer.step()\n",
    " \n",
    "        rollouts.after_update()\n",
    "\n",
    "        if j % args.save_interval == 0 and args.save_model:\n",
    "            save_model = actor_critic\n",
    "            if args.cuda:\n",
    "                save_model = copy.deepcopy(actor_critic).cpu()\n",
    "                \n",
    "            torch.save(actor_critic.state_dict(), SAVE_PATH)\n",
    "\n",
    "        if j % args.log_interval == 0:\n",
    "            end = time.time()\n",
    "            total_num_steps = (j + 1) * args.num_processes * args.num_steps\n",
    "            print(\"Updates {}, num timesteps {}, FPS {}, mean/median reward {:.1f}/{:.1f}, min/max reward {:.1f}/{:.1f}, entropy {:.5f}, value loss {:.5f}, policy loss {:.5f}\".\n",
    "                format(j, total_num_steps,\n",
    "                       int(total_num_steps / (end - start)),\n",
    "                       final_rewards.mean(),\n",
    "                       final_rewards.median(),\n",
    "                       final_rewards.min(),\n",
    "                       final_rewards.max(), dist_entropy.data[0],\n",
    "                       value_loss.data[0], action_loss.data[0]))\n",
    "            \n",
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cells are taken from the Baselines package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import orthogonal\n",
    "\n",
    "class Categorical(nn.Module):\n",
    "    def __init__(self, num_inputs, num_outputs):\n",
    "        super(Categorical, self).__init__()\n",
    "        self.linear = nn.Linear(num_inputs, num_outputs)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear(x)\n",
    "        return x\n",
    "\n",
    "    def sample(self, x, deterministic):\n",
    "        x = self(x)\n",
    "\n",
    "        probs = F.softmax(x)\n",
    "        if deterministic is False:\n",
    "            action = probs.multinomial()\n",
    "        else:\n",
    "            action = probs.max(1)[1]\n",
    "        return action\n",
    "\n",
    "    def logprobs_and_entropy(self, x, actions):\n",
    "        x = self(x)\n",
    "\n",
    "        log_probs = F.log_softmax(x)\n",
    "        probs = F.softmax(x)\n",
    "\n",
    "        action_log_probs = log_probs.gather(1, actions)\n",
    "\n",
    "        dist_entropy = -(log_probs * probs).sum(-1).mean()\n",
    "        return action_log_probs, dist_entropy\n",
    "    \n",
    "\n",
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1 or classname.find('Linear') != -1:\n",
    "        orthogonal(m.weight.data)\n",
    "        if m.bias is not None:\n",
    "            m.bias.data.fill_(0)\n",
    "\n",
    "\n",
    "class CNNPolicy(nn.Module):\n",
    "    def __init__(self, num_inputs, action_space):\n",
    "        super(CNNPolicy, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(num_inputs, 32, 8, stride=4)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 4, stride=2)\n",
    "        self.conv3 = nn.Conv2d(64, 32, 3, stride=1)\n",
    "\n",
    "        self.linear1 = nn.Linear(32 * 7 * 7, 512)\n",
    "\n",
    "        self.critic_linear = nn.Linear(512, 1)\n",
    "\n",
    "        num_outputs = action_space.n\n",
    "        self.dist = Categorical(512, num_outputs)\n",
    "\n",
    "        self.train() # training mode. Only affects dropout, batchnorm etc\n",
    "        self.reset_parameters()\n",
    "        \n",
    "    def act(self, inputs, states, masks, deterministic=False):\n",
    "        value, x, states = self(inputs, states, masks)\n",
    "        action = self.dist.sample(x, deterministic=deterministic)\n",
    "        action_log_probs, dist_entropy = self.dist.logprobs_and_entropy(x, action)\n",
    "        return value, action, action_log_probs, states\n",
    "\n",
    "    def evaluate_actions(self, inputs, states, masks, actions):\n",
    "        value, x, states = self(inputs, states, masks)\n",
    "        action_log_probs, dist_entropy = self.dist.logprobs_and_entropy(x, actions)\n",
    "        return value, action_log_probs, dist_entropy, states\n",
    "\n",
    "    @property\n",
    "    def state_size(self):\n",
    "        return 1\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        self.apply(weights_init)\n",
    "\n",
    "        relu_gain = nn.init.calculate_gain('relu')\n",
    "        self.conv1.weight.data.mul_(relu_gain)\n",
    "        self.conv2.weight.data.mul_(relu_gain)\n",
    "        self.conv3.weight.data.mul_(relu_gain)\n",
    "        self.linear1.weight.data.mul_(relu_gain)\n",
    "\n",
    "    def forward(self, inputs, states, masks):\n",
    "        x = self.conv1(inputs / 255.0)\n",
    "        x = F.relu(x)\n",
    "\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "\n",
    "        x = self.conv3(x)\n",
    "        x = F.relu(x)\n",
    "\n",
    "        x = x.view(-1, 32 * 7 * 7)\n",
    "        x = self.linear1(x)\n",
    "        x = F.relu(x)\n",
    "\n",
    "        return self.critic_linear(x), x, states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RolloutStorage(object):\n",
    "    def __init__(self, num_steps, num_processes, obs_shape, action_space, state_size):\n",
    "        self.observations = torch.zeros(num_steps + 1, num_processes, *obs_shape)\n",
    "        self.states = torch.zeros(num_steps + 1, num_processes, state_size)\n",
    "        self.rewards = torch.zeros(num_steps, num_processes, 1)\n",
    "        self.value_preds = torch.zeros(num_steps + 1, num_processes, 1)\n",
    "        self.returns = torch.zeros(num_steps + 1, num_processes, 1)\n",
    "        self.action_log_probs = torch.zeros(num_steps, num_processes, 1)\n",
    "        \n",
    "        action_shape = 1\n",
    "\n",
    "        self.actions = torch.zeros(num_steps, num_processes, action_shape)\n",
    "            \n",
    "        self.actions = self.actions.long()\n",
    "        self.masks = torch.ones(num_steps + 1, num_processes, 1)\n",
    "\n",
    "    def cuda(self):\n",
    "        self.observations = self.observations.cuda()\n",
    "        self.states = self.states.cuda()\n",
    "        self.rewards = self.rewards.cuda()\n",
    "        self.value_preds = self.value_preds.cuda()\n",
    "        self.returns = self.returns.cuda()\n",
    "        self.action_log_probs = self.action_log_probs.cuda()\n",
    "        self.actions = self.actions.cuda()\n",
    "        self.masks = self.masks.cuda()\n",
    "\n",
    "    def insert(self, step, current_obs, state, action, action_log_prob, value_pred, reward, mask):\n",
    "        self.observations[step + 1].copy_(current_obs)\n",
    "        self.states[step + 1].copy_(state)\n",
    "        self.actions[step].copy_(action)\n",
    "        self.action_log_probs[step].copy_(action_log_prob)\n",
    "        self.value_preds[step].copy_(value_pred)\n",
    "        self.rewards[step].copy_(reward)\n",
    "        self.masks[step + 1].copy_(mask)\n",
    "\n",
    "    def after_update(self):\n",
    "        self.observations[0].copy_(self.observations[-1])\n",
    "        self.states[0].copy_(self.states[-1])\n",
    "        self.masks[0].copy_(self.masks[-1])\n",
    "\n",
    "    def compute_returns(self, next_value, use_gae, gamma, tau):\n",
    "        self.returns[-1] = next_value\n",
    "        for step in reversed(range(self.rewards.size(0))):\n",
    "            self.returns[step] = self.returns[step + 1] * \\\n",
    "                gamma * self.masks[step + 1] + self.rewards[step]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import Process, Pipe\n",
    "\n",
    "def worker(remote, parent_remote, env_fn_wrapper):\n",
    "    parent_remote.close()\n",
    "    env = env_fn_wrapper.x()\n",
    "    while True:\n",
    "        cmd, data = remote.recv()\n",
    "        if cmd == 'step':\n",
    "            ob, reward, done, info = env.step(data)\n",
    "            if done:\n",
    "                ob = env.reset()\n",
    "            remote.send((ob, reward, done, info))\n",
    "        elif cmd == 'reset':\n",
    "            ob = env.reset()\n",
    "            remote.send(ob)\n",
    "        elif cmd == 'reset_task':\n",
    "            ob = env.reset_task()\n",
    "            remote.send(ob)\n",
    "        elif cmd == 'close':\n",
    "            remote.close()\n",
    "            break\n",
    "        elif cmd == 'get_spaces':\n",
    "            remote.send((env.action_space, env.observation_space))\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "\n",
    "class CloudpickleWrapper(object):\n",
    "    \"\"\"\n",
    "    Uses cloudpickle to serialize contents (otherwise multiprocessing tries to use pickle)\n",
    "    \"\"\"\n",
    "    def __init__(self, x):\n",
    "        self.x = x\n",
    "    def __getstate__(self):\n",
    "        import cloudpickle\n",
    "        return cloudpickle.dumps(self.x)\n",
    "    def __setstate__(self, ob):\n",
    "        import pickle\n",
    "        self.x = pickle.loads(ob)\n",
    "\n",
    "\n",
    "\n",
    "class SubprocVecEnv(object):\n",
    "    def __init__(self, env_fns):\n",
    "        \"\"\"\n",
    "        envs: list of gym environments to run in subprocesses\n",
    "        \"\"\"\n",
    "        self.closed = False\n",
    "        nenvs = len(env_fns)\n",
    "        self.remotes, self.work_remotes = zip(*[Pipe() for _ in range(nenvs)])\n",
    "        self.ps = [Process(target=worker, args=(work_remote, remote, CloudpickleWrapper(env_fn)))\n",
    "            for (work_remote, remote, env_fn) in zip(self.work_remotes, self.remotes, env_fns)]\n",
    "        for p in self.ps:\n",
    "            p.daemon = True # if the main process crashes, we should not cause things to hang\n",
    "            p.start()\n",
    "        for remote in self.work_remotes:\n",
    "            remote.close()\n",
    "\n",
    "        self.remotes[0].send(('get_spaces', None))\n",
    "        self.action_space, self.observation_space = self.remotes[0].recv()\n",
    "\n",
    "\n",
    "    def step(self, actions):\n",
    "        for remote, action in zip(self.remotes, actions):\n",
    "            remote.send(('step', action))\n",
    "        results = [remote.recv() for remote in self.remotes]\n",
    "        obs, rews, dones, infos = zip(*results)\n",
    "        return np.stack(obs), np.stack(rews), np.stack(dones), infos\n",
    "\n",
    "    def reset(self):\n",
    "        for remote in self.remotes:\n",
    "            remote.send(('reset', None))\n",
    "        return np.stack([remote.recv() for remote in self.remotes])\n",
    "\n",
    "    def reset_task(self):\n",
    "        for remote in self.remotes:\n",
    "            remote.send(('reset_task', None))\n",
    "        return np.stack([remote.recv() for remote in self.remotes])\n",
    "\n",
    "    def close(self):\n",
    "        if self.closed:\n",
    "            return\n",
    "\n",
    "        for remote in self.remotes:\n",
    "            remote.send(('close', None))\n",
    "        for p in self.ps:\n",
    "            p.join()\n",
    "        self.closed = True\n",
    "\n",
    "    @property\n",
    "    def num_envs(self):\n",
    "        return len(self.remotes)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
